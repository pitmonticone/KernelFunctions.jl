<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Train Kernel Parameters · KernelFunctions.jl</title><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.039/juliamono-regular.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.11/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../../">KernelFunctions.jl</a></span></div><form class="docs-search" action="../../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li><li><a class="tocitem" href="../../userguide/">User guide</a></li><li><a class="tocitem" href="../../kernels/">Kernel Functions</a></li><li><a class="tocitem" href="../../transform/">Input Transforms</a></li><li><a class="tocitem" href="../../metrics/">Metrics</a></li><li><a class="tocitem" href="../../create_kernel/">Custom Kernels</a></li><li><a class="tocitem" href="../../api/">API</a></li><li><a class="tocitem" href="../../design/">Design</a></li><li><span class="tocitem">Examples</span><ul><li><a class="tocitem" href="../deep-kernel-learning/">Deep Kernel Learning</a></li><li><a class="tocitem" href="../gaussian-process-priors/">Gaussian process prior samples</a></li><li><a class="tocitem" href="../kernel-ridge-regression/">Kernel Ridge Regression</a></li><li><a class="tocitem" href="../support-vector-machine/">Support Vector Machine</a></li><li class="is-active"><a class="tocitem" href>Train Kernel Parameters</a><ul class="internal"><li><a class="tocitem" href="#Data-Generation"><span>Data Generation</span></a></li><li><a class="tocitem" href="#Base-Approach"><span>Base Approach</span></a></li><li><a class="tocitem" href="#Using-ParameterHandling.jl"><span>Using ParameterHandling.jl</span></a></li><li><a class="tocitem" href="#Training-the-model"><span>Training the model</span></a></li><li><a class="tocitem" href="#Flux.destructure"><span>Flux.destructure</span></a></li><li><a class="tocitem" href="#Training-the-model-2"><span>Training the model</span></a></li></ul></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Examples</a></li><li class="is-active"><a href>Train Kernel Parameters</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Train Kernel Parameters</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/JuliaGaussianProcesses/KernelFunctions.jl/blob/master/examples/train-kernel-parameters/script.jl" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Train-Kernel-Parameters"><a class="docs-heading-anchor" href="#Train-Kernel-Parameters">Train Kernel Parameters</a><a id="Train-Kernel-Parameters-1"></a><a class="docs-heading-anchor-permalink" href="#Train-Kernel-Parameters" title="Permalink"></a></h1><p><a href="https://nbviewer.jupyter.org/github/JuliaGaussianProcesses/KernelFunctions.jl/blob/gh-pages/previews/PR430/examples/train-kernel-parameters.ipynb"><img src="https://img.shields.io/badge/show-nbviewer-579ACA.svg" alt/></a></p><p><em>You are seeing the HTML output generated by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and <a href="https://github.com/fredrikekre/Literate.jl">Literate.jl</a> from the <a href="https://github.com/JuliaGaussianProcesses/KernelFunctions.jl/blob/master/examples/train-kernel-parameters/script.jl">Julia source file</a>. The corresponding notebook can be viewed in <a href="https://nbviewer.jupyter.org/github/JuliaGaussianProcesses/KernelFunctions.jl/blob/gh-pages/previews/PR430/examples/train-kernel-parameters.ipynb">nbviewer</a>.</em></p><p>In this example we show a few ways to perform regression on a kernel from KernelFunctions.jl.</p><p>We load KernelFunctions and some other packages</p><pre><code class="language-julia hljs">using KernelFunctions
using LinearAlgebra
using Distributions
using Plots;
using BenchmarkTools
using Flux
using Flux: Optimise
using Zygote
using Random: seed!
seed!(42);</code></pre><h2 id="Data-Generation"><a class="docs-heading-anchor" href="#Data-Generation">Data Generation</a><a id="Data-Generation-1"></a><a class="docs-heading-anchor-permalink" href="#Data-Generation" title="Permalink"></a></h2><p>We generated data in 1 dimension</p><pre><code class="language-julia hljs">xmin = -3;
xmax = 3; # Bounds of the data
N = 50 # Number of samples
x_train = rand(Uniform(xmin, xmax), N) # We sample 100 random samples
σ = 0.1
y_train = sinc.(x_train) + randn(N) * σ # We create a function and add some noise
x_test = range(xmin - 0.1, xmax + 0.1; length=300)</code></pre><p>Plot the data</p><pre><code class="language-julia hljs">scatter(x_train, y_train; lab=&quot;data&quot;)
plot!(x_test, sinc; lab=&quot;true function&quot;)</code></pre><p><img src="../401463679.svg" alt/></p><h2 id="Base-Approach"><a class="docs-heading-anchor" href="#Base-Approach">Base Approach</a><a id="Base-Approach-1"></a><a class="docs-heading-anchor-permalink" href="#Base-Approach" title="Permalink"></a></h2><p>The first option is to rebuild the parametrized kernel from a vector of parameters in each evaluation of the cost fuction. This is similar to the approach taken in <a href="https://github.com/JuliaGaussianProcesses/Stheno.jl">Stheno.jl</a>.</p><p>To train the kernel parameters via ForwardDiff.jl we need to create a function creating a kernel from an array. A simple way to ensure that the kernel parameters are positive is to optimize over the logarithm of the parameters.</p><pre><code class="language-julia hljs">function kernelcall(θ)
    return (exp(θ[1]) * SqExponentialKernel() + exp(θ[2]) * Matern32Kernel()) ∘
           ScaleTransform(exp(θ[3]))
end</code></pre><p>From theory we know the prediction for a test set x given the kernel parameters and normalization constant</p><pre><code class="language-julia hljs">function f(x, x_train, y_train, θ)
    k = kernelcall(θ[1:3])
    return kernelmatrix(k, x, x_train) *
           ((kernelmatrix(k, x_train) + exp(θ[4]) * I) \ y_train)
end</code></pre><p>We look how the prediction looks like with starting parameters [1.0, 1.0, 1.0, 1.0] we get :</p><pre><code class="language-julia hljs">ŷ = f(x_test, x_train, y_train, log.(ones(4)))
scatter(x_train, y_train; lab=&quot;data&quot;)
plot!(x_test, sinc; lab=&quot;true function&quot;)
plot!(x_test, ŷ; lab=&quot;prediction&quot;)</code></pre><p><img src="../1527582623.svg" alt/></p><p>We define the loss based on the L2 norm both for the loss and the regularization</p><pre><code class="language-julia hljs">function loss(θ)
    ŷ = f(x_train, x_train, y_train, θ)
    return sum(abs2, y_train - ŷ) + exp(θ[4]) * norm(ŷ)
end</code></pre><h3 id="Training"><a class="docs-heading-anchor" href="#Training">Training</a><a id="Training-1"></a><a class="docs-heading-anchor-permalink" href="#Training" title="Permalink"></a></h3><p>Setting an initial value and initializing the optimizer:</p><pre><code class="language-julia hljs">θ = log.([1.1, 0.1, 0.01, 0.001]) # Initial vector
opt = Optimise.ADAGrad(0.5)</code></pre><p>The loss with our starting point:</p><pre><code class="language-julia hljs">loss(θ)</code></pre><pre><code class="nohighlight hljs">6.826611997832667</code></pre><p>Computational cost for one step</p><pre><code class="language-julia hljs">@benchmark let θt = θ[:], optt = Optimise.ADAGrad(0.5)
    grads = only((Zygote.gradient(loss, θt)))
    Optimise.update!(optt, θt, grads)
end</code></pre><pre><code class="nohighlight hljs">BenchmarkTools.Trial: 2901 samples with 1 evaluation.
 Range (min … max):  1.273 ms …   9.446 ms  ┊ GC (min … max): 0.00% … 71.33%
 Time  (median):     1.588 ms               ┊ GC (median):    0.00%
 Time  (mean ± σ):   1.716 ms ± 974.012 μs  ┊ GC (mean ± σ):  7.87% ± 11.17%

   ▁█▃                                                         
  ▄███▃▂▂▂▂▂▂▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▂ ▂
  1.27 ms         Histogram: frequency by time        8.44 ms &lt;

 Memory estimate: 1.96 MiB, allocs estimate: 1971.</code></pre><p>Optimizing</p><pre><code class="language-julia hljs">anim = Animation()
for i in 1:25
    grads = only((Zygote.gradient(loss, θ)))
    Optimise.update!(opt, θ, grads)
    scatter(
        x_train, y_train; lab=&quot;data&quot;, title=&quot;i = $(i), Loss = $(round(loss(θ), digits = 4))&quot;
    )
    plot!(x_test, sinc; lab=&quot;true function&quot;)
    plot!(x_test, f(x_test, x_train, y_train, θ); lab=&quot;Prediction&quot;, lw=3.0)
    frame(anim)
end
gif(anim, &quot;train-kernel-param.gif&quot;; show_msg=false, fps=15);</code></pre><p><img src="../train-kernel-param.gif" alt/></p><p>Final loss</p><pre><code class="language-julia hljs">loss(θ)</code></pre><pre><code class="nohighlight hljs">0.3115230878633516</code></pre><h2 id="Using-ParameterHandling.jl"><a class="docs-heading-anchor" href="#Using-ParameterHandling.jl">Using ParameterHandling.jl</a><a id="Using-ParameterHandling.jl-1"></a><a class="docs-heading-anchor-permalink" href="#Using-ParameterHandling.jl" title="Permalink"></a></h2><p>Alternatively, we can use the <a href="https://github.com/invenia/ParameterHandling.jl">ParameterHandling.jl</a> package to handle the requirement that all kernel parameters should be positive.</p><pre><code class="language-julia hljs">using ParameterHandling

raw_initial_θ = (
    k1=positive(1.1), k2=positive(0.1), k3=positive(0.01), noise_var=positive(0.001)
)

flat_θ, unflatten = ParameterHandling.value_flatten(raw_initial_θ)

function kernelcall(θ)
    return (θ.k1 * SqExponentialKernel() + θ.k2 * Matern32Kernel()) ∘ ScaleTransform(θ.k3)
end

function f(x, x_train, y_train, θ)
    k = kernelcall(θ)
    return kernelmatrix(k, x, x_train) *
           ((kernelmatrix(k, x_train) + θ.noise_var * I) \ y_train)
end

function loss(θ)
    ŷ = f(x_train, x_train, y_train, θ)
    return sum(abs2, y_train - ŷ) + θ.noise_var * norm(ŷ)
end

initial_θ = ParameterHandling.value(raw_initial_θ)</code></pre><p>The loss with our starting point :</p><pre><code class="language-julia hljs">(loss ∘ unflatten)(flat_θ)</code></pre><pre><code class="nohighlight hljs">6.826611997832667</code></pre><h2 id="Training-the-model"><a class="docs-heading-anchor" href="#Training-the-model">Training the model</a><a id="Training-the-model-1"></a><a class="docs-heading-anchor-permalink" href="#Training-the-model" title="Permalink"></a></h2><h3 id="Cost-per-step"><a class="docs-heading-anchor" href="#Cost-per-step">Cost per step</a><a id="Cost-per-step-1"></a><a class="docs-heading-anchor-permalink" href="#Cost-per-step" title="Permalink"></a></h3><pre><code class="language-julia hljs">@benchmark let θt = flat_θ[:], optt = Optimise.ADAGrad(0.5)
    grads = (Zygote.gradient(loss ∘ unflatten, θt))[1]
    Optimise.update!(optt, θt, grads)
end</code></pre><pre><code class="nohighlight hljs">BenchmarkTools.Trial: 2350 samples with 1 evaluation.
 Range (min … max):  1.519 ms … 11.283 ms  ┊ GC (min … max): 0.00% … 80.98%
 Time  (median):     1.909 ms              ┊ GC (median):    0.00%
 Time  (mean ± σ):   2.119 ms ±  1.222 ms  ┊ GC (mean ± σ):  7.52% ± 10.81%

  ▂▆█▅     ▂▁                                                 
  ████▅▃▅▇███▅▁▃▁▁▁▁▃▁▃▁▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▃▁▁▁▁▁▁▁▁▁▃▄▅▄▆▅ █
  1.52 ms      Histogram: log(frequency) by time     10.5 ms &lt;

 Memory estimate: 2.00 MiB, allocs estimate: 2739.</code></pre><h3 id="Complete-optimization"><a class="docs-heading-anchor" href="#Complete-optimization">Complete optimization</a><a id="Complete-optimization-1"></a><a class="docs-heading-anchor-permalink" href="#Complete-optimization" title="Permalink"></a></h3><pre><code class="language-julia hljs">opt = Optimise.ADAGrad(0.5)
for i in 1:25
    grads = (Zygote.gradient(loss ∘ unflatten, flat_θ))[1]
    Optimise.update!(opt, flat_θ, grads)
end</code></pre><p>Final loss</p><pre><code class="language-julia hljs">(loss ∘ unflatten)(flat_θ)</code></pre><pre><code class="nohighlight hljs">0.3115268205943201</code></pre><h2 id="Flux.destructure"><a class="docs-heading-anchor" href="#Flux.destructure">Flux.destructure</a><a id="Flux.destructure-1"></a><a class="docs-heading-anchor-permalink" href="#Flux.destructure" title="Permalink"></a></h2><p>If don&#39;t want to write an explicit function to construct the kernel, we can alternatively use the <code>Flux.destructure</code> function. Again, we need to ensure that the parameters are positive. Note that the <code>exp</code> function is now part of the loss function, instead of part of the kernel construction.</p><pre><code class="language-julia hljs">θ = [1.1, 0.1, 0.01, 0.001]

kernel = (θ[1] * SqExponentialKernel() + θ[2] * Matern32Kernel()) ∘ ScaleTransform(θ[3])

p, kernelc = Flux.destructure(kernel);</code></pre><p>This returns the <code>trainable</code> parameters of the kernel and a function to reconstruct the kernel.</p><pre><code class="language-julia hljs">kernelc(p)</code></pre><pre><code class="nohighlight hljs">Sum of 2 kernels:
	Squared Exponential Kernel (metric = Distances.Euclidean(0.0))
			- σ² = 1.1
	Matern 3/2 Kernel (metric = Distances.Euclidean(0.0))
			- σ² = 0.1
	- Scale Transform (s = 0.01)</code></pre><p>From theory we know the prediction for a test set x given the kernel parameters and normalization constant</p><pre><code class="language-julia hljs">function f(x, x_train, y_train, θ)
    k = kernelc(θ[1:3])
    return kernelmatrix(k, x, x_train) * ((kernelmatrix(k, x_train) + (θ[4]) * I) \ y_train)
end</code></pre><p>We define the loss based on the L2 norm both for the loss and the regularization</p><pre><code class="language-julia hljs">function loss(θ)
    ŷ = f(x_train, x_train, y_train, exp.(θ))
    return sum(abs2, y_train - ŷ) + exp(θ[4]) * norm(ŷ)
end</code></pre><h2 id="Training-the-model-2"><a class="docs-heading-anchor" href="#Training-the-model-2">Training the model</a><a class="docs-heading-anchor-permalink" href="#Training-the-model-2" title="Permalink"></a></h2><p>The loss with our starting point :</p><pre><code class="language-julia hljs">θ = log.([1.1, 0.1, 0.01, 0.001]) # Initial vector
loss(θ)</code></pre><pre><code class="nohighlight hljs">6.826611997832667</code></pre><p>Initialize optimizer</p><pre><code class="language-julia hljs">opt = Optimise.ADAGrad(0.5)</code></pre><p>Cost for one step</p><pre><code class="language-julia hljs">@benchmark let θt = θ[:], optt = Optimise.ADAGrad(0.5)
    grads = only((Zygote.gradient(loss, θt)))
    Optimise.update!(optt, θt, grads)
end</code></pre><pre><code class="nohighlight hljs">BenchmarkTools.Trial: 2710 samples with 1 evaluation.
 Range (min … max):  1.368 ms … 10.717 ms  ┊ GC (min … max): 0.00% … 74.80%
 Time  (median):     1.698 ms              ┊ GC (median):    0.00%
 Time  (mean ± σ):   1.836 ms ±  1.062 ms  ┊ GC (mean ± σ):  7.66% ± 10.80%

    █                                                         
  ▃██▅▂▂▁▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▂ ▂
  1.37 ms        Histogram: frequency by time        9.57 ms &lt;

 Memory estimate: 1.96 MiB, allocs estimate: 2054.</code></pre><p>The optimization</p><pre><code class="language-julia hljs">for i in 1:25
    grads = only((Zygote.gradient(loss, θ)))
    Optimise.update!(opt, θ, grads)
end</code></pre><p>Final loss</p><pre><code class="language-julia hljs">loss(θ)</code></pre><pre><code class="nohighlight hljs">0.3115230878633516</code></pre><hr/><p><em>This page was generated using <a href="https://github.com/fredrikekre/Literate.jl">Literate.jl</a>.</em></p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../support-vector-machine/">« Support Vector Machine</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.10 on <span class="colophon-date" title="Saturday 29 January 2022 21:35">Saturday 29 January 2022</span>. Using Julia version 1.7.1.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
