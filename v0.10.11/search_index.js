var documenterSearchIndex = {"docs":
[{"location":"examples/support-vector-machine/","page":"Support Vector Machine","title":"Support Vector Machine","text":"EditURL = \"https://github.com/JuliaGaussianProcesses/KernelFunctions.jl/blob/master/examples/support-vector-machine/script.jl\"","category":"page"},{"location":"examples/support-vector-machine/#Support-Vector-Machine","page":"Support Vector Machine","title":"Support Vector Machine","text":"","category":"section"},{"location":"examples/support-vector-machine/","page":"Support Vector Machine","title":"Support Vector Machine","text":"(Image: )","category":"page"},{"location":"examples/support-vector-machine/","page":"Support Vector Machine","title":"Support Vector Machine","text":"You are seeing the HTML output generated by Documenter.jl and Literate.jl from the Julia source file. The corresponding notebook can be viewed in nbviewer.","category":"page"},{"location":"examples/support-vector-machine/","page":"Support Vector Machine","title":"Support Vector Machine","text":"using Distributions\nusing KernelFunctions\nusing LIBSVM\nusing LinearAlgebra\nusing Plots\nusing Random\n\n# Set plotting theme\ntheme(:wong)\n\n# Set seed\nRandom.seed!(1234);","category":"page"},{"location":"examples/support-vector-machine/","page":"Support Vector Machine","title":"Support Vector Machine","text":"Number of samples:","category":"page"},{"location":"examples/support-vector-machine/","page":"Support Vector Machine","title":"Support Vector Machine","text":"N = 100;","category":"page"},{"location":"examples/support-vector-machine/","page":"Support Vector Machine","title":"Support Vector Machine","text":"Select randomly between two classes:","category":"page"},{"location":"examples/support-vector-machine/","page":"Support Vector Machine","title":"Support Vector Machine","text":"y_train = rand([-1, 1], N);","category":"page"},{"location":"examples/support-vector-machine/","page":"Support Vector Machine","title":"Support Vector Machine","text":"Random attributes for both classes:","category":"page"},{"location":"examples/support-vector-machine/","page":"Support Vector Machine","title":"Support Vector Machine","text":"X = Matrix{Float64}(undef, 2, N)\nrand!(MvNormal(randn(2), I), view(X, :, y_train .== 1))\nrand!(MvNormal(randn(2), I), view(X, :, y_train .== -1));\nx_train = ColVecs(X);","category":"page"},{"location":"examples/support-vector-machine/","page":"Support Vector Machine","title":"Support Vector Machine","text":"Create a 2D grid:","category":"page"},{"location":"examples/support-vector-machine/","page":"Support Vector Machine","title":"Support Vector Machine","text":"test_range = range(floor(Int, minimum(X)), ceil(Int, maximum(X)); length=100)\nx_test = ColVecs(mapreduce(collect, hcat, Iterators.product(test_range, test_range)));","category":"page"},{"location":"examples/support-vector-machine/","page":"Support Vector Machine","title":"Support Vector Machine","text":"Create kernel function:","category":"page"},{"location":"examples/support-vector-machine/","page":"Support Vector Machine","title":"Support Vector Machine","text":"k = SqExponentialKernel() ∘ ScaleTransform(2.0)","category":"page"},{"location":"examples/support-vector-machine/","page":"Support Vector Machine","title":"Support Vector Machine","text":"Squared Exponential Kernel (metric = Distances.Euclidean(0.0))\n\t- Scale Transform (s = 2.0)","category":"page"},{"location":"examples/support-vector-machine/","page":"Support Vector Machine","title":"Support Vector Machine","text":"LIBSVM can make use of a pre-computed kernel matrix. KernelFunctions.jl can be used to produce that. Precomputed matrix for training (corresponds to linear kernel)","category":"page"},{"location":"examples/support-vector-machine/","page":"Support Vector Machine","title":"Support Vector Machine","text":"model = svmtrain(kernelmatrix(k, x_train), y_train; kernel=LIBSVM.Kernel.Precomputed)","category":"page"},{"location":"examples/support-vector-machine/","page":"Support Vector Machine","title":"Support Vector Machine","text":"LIBSVM.SVM{Int64}(LIBSVM.SVC, LIBSVM.Kernel.Precomputed, nothing, 1, 2, [-1, 1], Int32[1, 2], Float64[], Int32[], LIBSVM.SupportVectors{Vector{Int64}, Matrix{Int64}}(66, Int32[34, 32], [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1 5 6 9 11 19 20 22 24 31 38 41 42 43 45 46 48 50 52 53 54 62 68 70 71 72 77 84 86 89 90 92 97 98 2 3 7 10 13 14 16 17 28 33 35 36 44 49 58 60 63 64 65 66 69 74 79 81 82 87 88 91 93 94 95 100], Int32[1, 5, 6, 9, 11, 19, 20, 22, 24, 31, 38, 41, 42, 43, 45, 46, 48, 50, 52, 53, 54, 62, 68, 70, 71, 72, 77, 84, 86, 89, 90, 92, 97, 98, 2, 3, 7, 10, 13, 14, 16, 17, 28, 33, 35, 36, 44, 49, 58, 60, 63, 64, 65, 66, 69, 74, 79, 81, 82, 87, 88, 91, 93, 94, 95, 100], LIBSVM.SVMNode[LIBSVM.SVMNode(0, 1.0), LIBSVM.SVMNode(0, 5.0), LIBSVM.SVMNode(0, 6.0), LIBSVM.SVMNode(0, 9.0), LIBSVM.SVMNode(0, 11.0), LIBSVM.SVMNode(0, 19.0), LIBSVM.SVMNode(0, 20.0), LIBSVM.SVMNode(0, 22.0), LIBSVM.SVMNode(0, 24.0), LIBSVM.SVMNode(0, 31.0), LIBSVM.SVMNode(0, 38.0), LIBSVM.SVMNode(0, 41.0), LIBSVM.SVMNode(0, 42.0), LIBSVM.SVMNode(0, 43.0), LIBSVM.SVMNode(0, 45.0), LIBSVM.SVMNode(0, 46.0), LIBSVM.SVMNode(0, 48.0), LIBSVM.SVMNode(0, 50.0), LIBSVM.SVMNode(0, 52.0), LIBSVM.SVMNode(0, 53.0), LIBSVM.SVMNode(0, 54.0), LIBSVM.SVMNode(0, 62.0), LIBSVM.SVMNode(0, 68.0), LIBSVM.SVMNode(0, 70.0), LIBSVM.SVMNode(0, 71.0), LIBSVM.SVMNode(0, 72.0), LIBSVM.SVMNode(0, 77.0), LIBSVM.SVMNode(0, 84.0), LIBSVM.SVMNode(0, 86.0), LIBSVM.SVMNode(0, 89.0), LIBSVM.SVMNode(0, 90.0), LIBSVM.SVMNode(0, 92.0), LIBSVM.SVMNode(0, 97.0), LIBSVM.SVMNode(0, 98.0), LIBSVM.SVMNode(0, 2.0), LIBSVM.SVMNode(0, 3.0), LIBSVM.SVMNode(0, 7.0), LIBSVM.SVMNode(0, 10.0), LIBSVM.SVMNode(0, 13.0), LIBSVM.SVMNode(0, 14.0), LIBSVM.SVMNode(0, 16.0), LIBSVM.SVMNode(0, 17.0), LIBSVM.SVMNode(0, 28.0), LIBSVM.SVMNode(0, 33.0), LIBSVM.SVMNode(0, 35.0), LIBSVM.SVMNode(0, 36.0), LIBSVM.SVMNode(0, 44.0), LIBSVM.SVMNode(0, 49.0), LIBSVM.SVMNode(0, 58.0), LIBSVM.SVMNode(0, 60.0), LIBSVM.SVMNode(0, 63.0), LIBSVM.SVMNode(0, 64.0), LIBSVM.SVMNode(0, 65.0), LIBSVM.SVMNode(0, 66.0), LIBSVM.SVMNode(0, 69.0), LIBSVM.SVMNode(0, 74.0), LIBSVM.SVMNode(0, 79.0), LIBSVM.SVMNode(0, 81.0), LIBSVM.SVMNode(0, 82.0), LIBSVM.SVMNode(0, 87.0), LIBSVM.SVMNode(0, 88.0), LIBSVM.SVMNode(0, 91.0), LIBSVM.SVMNode(0, 93.0), LIBSVM.SVMNode(0, 94.0), LIBSVM.SVMNode(0, 95.0), LIBSVM.SVMNode(0, 100.0)]), 0.0, [0.0011515178246988868; 1.0; 1.0; 0.8930493877799983; 0.2641190263588175; 1.0; 0.5244764472516993; 1.0; 1.0; 1.0; 0.8905706185492047; 0.3295945458044202; 1.0; 0.860978815089442; 0.06683377730458326; 1.0; 0.5367171980111412; 0.6534975532149773; 0.08305132933896073; 0.39430290196152556; 0.8926136440135181; 1.0; 0.7293682767212791; 1.0; 0.5286072085658116; 1.0; 1.0; 1.0; 1.0; 0.7409431996238991; 1.0; 0.5185305984259766; 0.12998947221457693; 0.47427088502460657; -0.8331283226322113; -0.5010491777775455; -1.0; -0.7808830607197881; -0.16244354814614123; -1.0; -1.0; -1.0; -1.0; -0.7215475396879995; -1.0; -0.13281444073688553; -1.0; -0.24528817001444883; -1.0; -0.5591078904088818; -1.0; -0.3683419909655691; -0.14057533868554514; -0.7998366322570258; -1.0; -1.0; -0.9120643791060599; -1.0; -1.0; -1.0; -1.0; -0.34875169623048013; -0.006834215710554889; -1.0; -1.0; -1.0], Float64[], Float64[], [-0.10543655890804106], 3, 0.01, 200.0, 0.001, 1.0, 0.5, 0.1, true, false)","category":"page"},{"location":"examples/support-vector-machine/","page":"Support Vector Machine","title":"Support Vector Machine","text":"Precomputed matrix for prediction","category":"page"},{"location":"examples/support-vector-machine/","page":"Support Vector Machine","title":"Support Vector Machine","text":"y_pr, _ = svmpredict(model, kernelmatrix(k, x_train, x_test));","category":"page"},{"location":"examples/support-vector-machine/","page":"Support Vector Machine","title":"Support Vector Machine","text":"Compute prediction on a grid:","category":"page"},{"location":"examples/support-vector-machine/","page":"Support Vector Machine","title":"Support Vector Machine","text":"contourf(test_range, test_range, y_pr)\nscatter!(X[1, :], X[2, :]; color=y_train, lab=\"data\", widen=false)","category":"page"},{"location":"examples/support-vector-machine/","page":"Support Vector Machine","title":"Support Vector Machine","text":"(Image: )","category":"page"},{"location":"examples/support-vector-machine/","page":"Support Vector Machine","title":"Support Vector Machine","text":"","category":"page"},{"location":"examples/support-vector-machine/","page":"Support Vector Machine","title":"Support Vector Machine","text":"This page was generated using Literate.jl.","category":"page"},{"location":"create_kernel/#Custom-Kernels","page":"Custom Kernels","title":"Custom Kernels","text":"","category":"section"},{"location":"create_kernel/#Creating-your-own-kernel","page":"Custom Kernels","title":"Creating your own kernel","text":"","category":"section"},{"location":"create_kernel/","page":"Custom Kernels","title":"Custom Kernels","text":"KernelFunctions.jl contains the most popular kernels already but you might want to make your own!","category":"page"},{"location":"create_kernel/","page":"Custom Kernels","title":"Custom Kernels","text":"Here are a few ways depending on how complicated your kernel is:","category":"page"},{"location":"create_kernel/#SimpleKernel-for-kernel-functions-depending-on-a-metric","page":"Custom Kernels","title":"SimpleKernel for kernel functions depending on a metric","text":"","category":"section"},{"location":"create_kernel/","page":"Custom Kernels","title":"Custom Kernels","text":"If your kernel function is of the form k(x, y) = f(d(x, y)) where d(x, y) is a PreMetric, you can construct your custom kernel by defining kappa and metric for your kernel. Here is for example how one can define the SqExponentialKernel again:","category":"page"},{"location":"create_kernel/","page":"Custom Kernels","title":"Custom Kernels","text":"struct MyKernel <: KernelFunctions.SimpleKernel end\n\nKernelFunctions.kappa(::MyKernel, d2::Real) = exp(-d2)\nKernelFunctions.metric(::MyKernel) = SqEuclidean()","category":"page"},{"location":"create_kernel/#Kernel-for-more-complex-kernels","page":"Custom Kernels","title":"Kernel for more complex kernels","text":"","category":"section"},{"location":"create_kernel/","page":"Custom Kernels","title":"Custom Kernels","text":"If your kernel does not satisfy such a representation, all you need to do is define (k::MyKernel)(x, y) and inherit from Kernel. For example, we recreate here the NeuralNetworkKernel:","category":"page"},{"location":"create_kernel/","page":"Custom Kernels","title":"Custom Kernels","text":"struct MyKernel <: KernelFunctions.Kernel end\n\n(::MyKernel)(x, y) = asin(dot(x, y) / sqrt((1 + sum(abs2, x)) * (1 + sum(abs2, y))))","category":"page"},{"location":"create_kernel/","page":"Custom Kernels","title":"Custom Kernels","text":"Note that the fallback implementation of the base Kernel evaluation does not use Distances.jl and can therefore be a bit slower.","category":"page"},{"location":"create_kernel/#Additional-Options","page":"Custom Kernels","title":"Additional Options","text":"","category":"section"},{"location":"create_kernel/","page":"Custom Kernels","title":"Custom Kernels","text":"Finally there are additional functions you can define to bring in more features:","category":"page"},{"location":"create_kernel/","page":"Custom Kernels","title":"Custom Kernels","text":"KernelFunctions.iskroncompatible(k::MyKernel): if your kernel factorizes in dimensions, you can declare your kernel as iskroncompatible(k) = true to use Kronecker methods.\nKernelFunctions.dim(x::MyDataType): by default the dimension of the inputs will only be checked for vectors of type AbstractVector{<:Real}. If you want to check the dimensionality of your inputs, dispatch the dim function on your datatype. Note that 0 is the default.\ndim is called within KernelFunctions.validate_inputs(x::MyDataType, y::MyDataType), which can instead be directly overloaded if you want to run special checks for your input types.\nkernelmatrix(k::MyKernel, ...): you can redefine the diverse kernelmatrix functions to eventually optimize the computations.\nBase.print(io::IO, k::MyKernel): if you want to specialize the printing of your kernel.","category":"page"},{"location":"create_kernel/","page":"Custom Kernels","title":"Custom Kernels","text":"KernelFunctions uses Functors.jl for specifying trainable kernel parameters in a way that is compatible with the Flux ML framework. You can use Functors.@functor if all fields of your kernel struct are trainable. Note that optimization algorithms in Flux are not compatible with scalar parameters (yet), and hence vector-valued parameters should be preferred.","category":"page"},{"location":"create_kernel/","page":"Custom Kernels","title":"Custom Kernels","text":"import Functors\n\nstruct MyKernel{T} <: KernelFunctions.Kernel\n    a::Vector{T}\nend\n\nFunctors.@functor MyKernel","category":"page"},{"location":"create_kernel/","page":"Custom Kernels","title":"Custom Kernels","text":"If only a subset of the fields are trainable, you have to specify explicitly how to (re)construct the kernel with modified parameter values by implementing Functors.functor(::Type{<:MyKernel}, x) for your kernel struct:","category":"page"},{"location":"create_kernel/","page":"Custom Kernels","title":"Custom Kernels","text":"import Functors\n\nstruct MyKernel{T} <: KernelFunctions.Kernel\n    n::Int\n    a::Vector{T}\nend\n\nfunction Functors.functor(::Type{<:MyKernel}, x::MyKernel)\n    function reconstruct_mykernel(xs)\n        # keep field `n` of the original kernel and set `a` to (possibly different) `xs.a`\n        return MyKernel(x.n, xs.a)\n    end\n    return (a = x.a,), reconstruct_mykernel\nend","category":"page"},{"location":"examples/kernel-ridge-regression/","page":"Kernel Ridge Regression","title":"Kernel Ridge Regression","text":"EditURL = \"https://github.com/JuliaGaussianProcesses/KernelFunctions.jl/blob/master/examples/kernel-ridge-regression/script.jl\"","category":"page"},{"location":"examples/kernel-ridge-regression/#Kernel-Ridge-Regression","page":"Kernel Ridge Regression","title":"Kernel Ridge Regression","text":"","category":"section"},{"location":"examples/kernel-ridge-regression/","page":"Kernel Ridge Regression","title":"Kernel Ridge Regression","text":"(Image: )","category":"page"},{"location":"examples/kernel-ridge-regression/","page":"Kernel Ridge Regression","title":"Kernel Ridge Regression","text":"You are seeing the HTML output generated by Documenter.jl and Literate.jl from the Julia source file. The corresponding notebook can be viewed in nbviewer.","category":"page"},{"location":"examples/kernel-ridge-regression/","page":"Kernel Ridge Regression","title":"Kernel Ridge Regression","text":"Building on linear regression, we can fit non-linear data sets by introducing a feature space. In a higher-dimensional feature space, we can overfit the data; ridge regression introduces regularization to avoid this. In this notebook we show how we can use KernelFunctions.jl for kernel ridge regression.","category":"page"},{"location":"examples/kernel-ridge-regression/","page":"Kernel Ridge Regression","title":"Kernel Ridge Regression","text":"# Loading and setup of required packages\nusing KernelFunctions\nusing LinearAlgebra\nusing Distributions\n\n# Plotting\nusing Plots;\ndefault(; lw=2.0, legendfontsize=11.0, ylims=(-150, 500));\n\nusing Random: seed!\nseed!(42);","category":"page"},{"location":"examples/kernel-ridge-regression/#Toy-data","page":"Kernel Ridge Regression","title":"Toy data","text":"","category":"section"},{"location":"examples/kernel-ridge-regression/","page":"Kernel Ridge Regression","title":"Kernel Ridge Regression","text":"Here we use a one-dimensional toy problem. We generate data using the fourth-order polynomial f(x) = (x+4)(x+1)(x-1)(x-3):","category":"page"},{"location":"examples/kernel-ridge-regression/","page":"Kernel Ridge Regression","title":"Kernel Ridge Regression","text":"f_truth(x) = (x + 4) * (x + 1) * (x - 1) * (x - 3)\n\nx_train = -5:0.5:5\nx_test = -7:0.1:7\n\nnoise = rand(Uniform(-20, 20), length(x_train))\ny_train = f_truth.(x_train) + noise\ny_test = f_truth.(x_test)\n\nplot(x_test, y_test; label=raw\"$f(x)$\")\nscatter!(x_train, y_train; seriescolor=1, label=\"observations\")","category":"page"},{"location":"examples/kernel-ridge-regression/","page":"Kernel Ridge Regression","title":"Kernel Ridge Regression","text":"(Image: )","category":"page"},{"location":"examples/kernel-ridge-regression/#Linear-regression","page":"Kernel Ridge Regression","title":"Linear regression","text":"","category":"section"},{"location":"examples/kernel-ridge-regression/","page":"Kernel Ridge Regression","title":"Kernel Ridge Regression","text":"For training inputs mathrmX=(mathbfx_n)_n=1^N and observations mathbfy=(y_n)_n=1^N, the linear regression weights mathbfw using the least-squares estimator are given by","category":"page"},{"location":"examples/kernel-ridge-regression/","page":"Kernel Ridge Regression","title":"Kernel Ridge Regression","text":"mathbfw = (mathrmX^top mathrmX)^-1 mathrmX^top mathbfy","category":"page"},{"location":"examples/kernel-ridge-regression/","page":"Kernel Ridge Regression","title":"Kernel Ridge Regression","text":"We predict at test inputs mathbfx_* using","category":"page"},{"location":"examples/kernel-ridge-regression/","page":"Kernel Ridge Regression","title":"Kernel Ridge Regression","text":"haty_* = mathbfx_*^top mathbfw","category":"page"},{"location":"examples/kernel-ridge-regression/","page":"Kernel Ridge Regression","title":"Kernel Ridge Regression","text":"This is implemented by linear_regression:","category":"page"},{"location":"examples/kernel-ridge-regression/","page":"Kernel Ridge Regression","title":"Kernel Ridge Regression","text":"function linear_regression(X, y, Xstar)\n    weights = (X' * X) \\ (X' * y)\n    return Xstar * weights\nend;","category":"page"},{"location":"examples/kernel-ridge-regression/","page":"Kernel Ridge Regression","title":"Kernel Ridge Regression","text":"A linear regression fit to the above data set:","category":"page"},{"location":"examples/kernel-ridge-regression/","page":"Kernel Ridge Regression","title":"Kernel Ridge Regression","text":"y_pred = linear_regression(x_train, y_train, x_test)\nscatter(x_train, y_train; label=\"observations\")\nplot!(x_test, y_pred; label=\"linear fit\")","category":"page"},{"location":"examples/kernel-ridge-regression/","page":"Kernel Ridge Regression","title":"Kernel Ridge Regression","text":"(Image: )","category":"page"},{"location":"examples/kernel-ridge-regression/#Featurization","page":"Kernel Ridge Regression","title":"Featurization","text":"","category":"section"},{"location":"examples/kernel-ridge-regression/","page":"Kernel Ridge Regression","title":"Kernel Ridge Regression","text":"We can improve the fit by including additional features, i.e. generalizing to tildemathrmX = (phi(x_n))_n=1^N, where phi(x) constructs a feature vector for each input x. Here we include powers of the input, phi(x) = (1 x x^2 dots x^d):","category":"page"},{"location":"examples/kernel-ridge-regression/","page":"Kernel Ridge Regression","title":"Kernel Ridge Regression","text":"function featurize_poly(x; degree=1)\n    return repeat(x, 1, degree + 1) .^ (0:degree)'\nend\n\nfunction featurized_fit_and_plot(degree)\n    X = featurize_poly(x_train; degree=degree)\n    Xstar = featurize_poly(x_test; degree=degree)\n    y_pred = linear_regression(X, y_train, Xstar)\n    scatter(x_train, y_train; legend=false, title=\"fit of order $degree\")\n    return plot!(x_test, y_pred)\nend\n\nplot((featurized_fit_and_plot(degree) for degree in 1:4)...)","category":"page"},{"location":"examples/kernel-ridge-regression/","page":"Kernel Ridge Regression","title":"Kernel Ridge Regression","text":"(Image: )","category":"page"},{"location":"examples/kernel-ridge-regression/","page":"Kernel Ridge Regression","title":"Kernel Ridge Regression","text":"Note that the fit becomes perfect when we include exactly as many orders in the features as we have in the underlying polynomial (4).","category":"page"},{"location":"examples/kernel-ridge-regression/","page":"Kernel Ridge Regression","title":"Kernel Ridge Regression","text":"However, when increasing the number of features, we can quickly overfit to noise in the data set:","category":"page"},{"location":"examples/kernel-ridge-regression/","page":"Kernel Ridge Regression","title":"Kernel Ridge Regression","text":"featurized_fit_and_plot(20)","category":"page"},{"location":"examples/kernel-ridge-regression/","page":"Kernel Ridge Regression","title":"Kernel Ridge Regression","text":"(Image: )","category":"page"},{"location":"examples/kernel-ridge-regression/#Ridge-regression","page":"Kernel Ridge Regression","title":"Ridge regression","text":"","category":"section"},{"location":"examples/kernel-ridge-regression/","page":"Kernel Ridge Regression","title":"Kernel Ridge Regression","text":"To counteract this unwanted behaviour, we can introduce regularization. This leads to ridge regression with L_2 regularization of the weights (Tikhonov regularization). Instead of the weights in linear regression,","category":"page"},{"location":"examples/kernel-ridge-regression/","page":"Kernel Ridge Regression","title":"Kernel Ridge Regression","text":"mathbfw = (mathrmX^top mathrmX)^-1 mathrmX^top mathbfy","category":"page"},{"location":"examples/kernel-ridge-regression/","page":"Kernel Ridge Regression","title":"Kernel Ridge Regression","text":"we introduce the ridge parameter lambda:","category":"page"},{"location":"examples/kernel-ridge-regression/","page":"Kernel Ridge Regression","title":"Kernel Ridge Regression","text":"mathbfw = (mathrmX^top mathrmX + lambda mathbb1)^-1 mathrmX^top mathbfy","category":"page"},{"location":"examples/kernel-ridge-regression/","page":"Kernel Ridge Regression","title":"Kernel Ridge Regression","text":"As before, we predict at test inputs mathbfx_* using","category":"page"},{"location":"examples/kernel-ridge-regression/","page":"Kernel Ridge Regression","title":"Kernel Ridge Regression","text":"haty_* = mathbfx_*^top mathbfw","category":"page"},{"location":"examples/kernel-ridge-regression/","page":"Kernel Ridge Regression","title":"Kernel Ridge Regression","text":"This is implemented by ridge_regression:","category":"page"},{"location":"examples/kernel-ridge-regression/","page":"Kernel Ridge Regression","title":"Kernel Ridge Regression","text":"function ridge_regression(X, y, Xstar, lambda)\n    weights = (X' * X + lambda * I) \\ (X' * y)\n    return Xstar * weights\nend\n\nfunction regularized_fit_and_plot(degree, lambda)\n    X = featurize_poly(x_train; degree=degree)\n    Xstar = featurize_poly(x_test; degree=degree)\n    y_pred = ridge_regression(X, y_train, Xstar, lambda)\n    scatter(x_train, y_train; legend=false, title=\"\\$\\\\lambda=$lambda\\$\")\n    return plot!(x_test, y_pred)\nend\n\nplot((regularized_fit_and_plot(20, lambda) for lambda in (1e-3, 1e-2, 1e-1, 1))...)","category":"page"},{"location":"examples/kernel-ridge-regression/","page":"Kernel Ridge Regression","title":"Kernel Ridge Regression","text":"(Image: )","category":"page"},{"location":"examples/kernel-ridge-regression/#Kernel-ridge-regression","page":"Kernel Ridge Regression","title":"Kernel ridge regression","text":"","category":"section"},{"location":"examples/kernel-ridge-regression/","page":"Kernel Ridge Regression","title":"Kernel Ridge Regression","text":"Instead of constructing the feature matrix explicitly, we can use kernels to replace inner products of feature vectors with a kernel evaluation: langle phi(x) phi(x) rangle = k(x x) or tildemathrmX tildemathrmX^top = mathrmK, where mathrmK_ij = k(x_i x_j).","category":"page"},{"location":"examples/kernel-ridge-regression/","page":"Kernel Ridge Regression","title":"Kernel Ridge Regression","text":"To apply this \"kernel trick\" to ridge regression, we can rewrite the ridge estimate for the weights","category":"page"},{"location":"examples/kernel-ridge-regression/","page":"Kernel Ridge Regression","title":"Kernel Ridge Regression","text":"mathbfw = (mathrmX^top mathrmX + lambda mathbb1)^-1 mathrmX^top mathbfy","category":"page"},{"location":"examples/kernel-ridge-regression/","page":"Kernel Ridge Regression","title":"Kernel Ridge Regression","text":"using the matrix inversion lemma as","category":"page"},{"location":"examples/kernel-ridge-regression/","page":"Kernel Ridge Regression","title":"Kernel Ridge Regression","text":"mathbfw = mathrmX^top (mathrmX mathrmX^top + lambda mathbb1)^-1 mathbfy","category":"page"},{"location":"examples/kernel-ridge-regression/","page":"Kernel Ridge Regression","title":"Kernel Ridge Regression","text":"where we can now replace the inner product with the kernel matrix,","category":"page"},{"location":"examples/kernel-ridge-regression/","page":"Kernel Ridge Regression","title":"Kernel Ridge Regression","text":"mathbfw = mathrmX^top (mathrmK + lambda mathbb1)^-1 mathbfy","category":"page"},{"location":"examples/kernel-ridge-regression/","page":"Kernel Ridge Regression","title":"Kernel Ridge Regression","text":"And the prediction yields another inner product,","category":"page"},{"location":"examples/kernel-ridge-regression/","page":"Kernel Ridge Regression","title":"Kernel Ridge Regression","text":"haty_* = mathbfx_*^top mathbfw = langle mathbfx_* mathbfw rangle = mathbfk_* (mathrmK + lambda mathbb1)^-1 mathbfy","category":"page"},{"location":"examples/kernel-ridge-regression/","page":"Kernel Ridge Regression","title":"Kernel Ridge Regression","text":"where (mathbfk_*)_n = k(x_* x_n).","category":"page"},{"location":"examples/kernel-ridge-regression/","page":"Kernel Ridge Regression","title":"Kernel Ridge Regression","text":"This is implemented by kernel_ridge_regression:","category":"page"},{"location":"examples/kernel-ridge-regression/","page":"Kernel Ridge Regression","title":"Kernel Ridge Regression","text":"function kernel_ridge_regression(k, X, y, Xstar, lambda)\n    K = kernelmatrix(k, X)\n    kstar = kernelmatrix(k, Xstar, X)\n    return kstar * ((K + lambda * I) \\ y)\nend;","category":"page"},{"location":"examples/kernel-ridge-regression/","page":"Kernel Ridge Regression","title":"Kernel Ridge Regression","text":"Now, instead of explicitly constructing features, we can simply pass in a PolynomialKernel object:","category":"page"},{"location":"examples/kernel-ridge-regression/","page":"Kernel Ridge Regression","title":"Kernel Ridge Regression","text":"function kernelized_fit_and_plot(kernel, lambda=1e-4)\n    y_pred = kernel_ridge_regression(kernel, x_train, y_train, x_test, lambda)\n    if kernel isa PolynomialKernel\n        title = string(\"order \", kernel.degree)\n    else\n        title = string(nameof(typeof(kernel)))\n    end\n    scatter(x_train, y_train; label=nothing)\n    return plot!(x_test, y_pred; label=nothing, title=title)\nend\n\nplot((kernelized_fit_and_plot(PolynomialKernel(; degree=degree, c=1)) for degree in 1:4)...)","category":"page"},{"location":"examples/kernel-ridge-regression/","page":"Kernel Ridge Regression","title":"Kernel Ridge Regression","text":"(Image: )","category":"page"},{"location":"examples/kernel-ridge-regression/","page":"Kernel Ridge Regression","title":"Kernel Ridge Regression","text":"However, we can now also use kernels that would have an infinite-dimensional feature expansion, such as the squared exponential kernel:","category":"page"},{"location":"examples/kernel-ridge-regression/","page":"Kernel Ridge Regression","title":"Kernel Ridge Regression","text":"kernelized_fit_and_plot(SqExponentialKernel())","category":"page"},{"location":"examples/kernel-ridge-regression/","page":"Kernel Ridge Regression","title":"Kernel Ridge Regression","text":"(Image: )","category":"page"},{"location":"examples/kernel-ridge-regression/","page":"Kernel Ridge Regression","title":"Kernel Ridge Regression","text":"","category":"page"},{"location":"examples/kernel-ridge-regression/","page":"Kernel Ridge Regression","title":"Kernel Ridge Regression","text":"This page was generated using Literate.jl.","category":"page"},{"location":"examples/gaussian-process-priors/","page":"Gaussian process prior samples","title":"Gaussian process prior samples","text":"EditURL = \"https://github.com/JuliaGaussianProcesses/KernelFunctions.jl/blob/master/examples/gaussian-process-priors/script.jl\"","category":"page"},{"location":"examples/gaussian-process-priors/#Gaussian-process-prior-samples","page":"Gaussian process prior samples","title":"Gaussian process prior samples","text":"","category":"section"},{"location":"examples/gaussian-process-priors/","page":"Gaussian process prior samples","title":"Gaussian process prior samples","text":"(Image: )","category":"page"},{"location":"examples/gaussian-process-priors/","page":"Gaussian process prior samples","title":"Gaussian process prior samples","text":"You are seeing the HTML output generated by Documenter.jl and Literate.jl from the Julia source file. The corresponding notebook can be viewed in nbviewer.","category":"page"},{"location":"examples/gaussian-process-priors/","page":"Gaussian process prior samples","title":"Gaussian process prior samples","text":"The kernels defined in this package can also be used to specify the covariance of a Gaussian process prior. A Gaussian process (GP) is defined by its mean function m(cdot) and its covariance function or kernel k(cdot cdot):","category":"page"},{"location":"examples/gaussian-process-priors/","page":"Gaussian process prior samples","title":"Gaussian process prior samples","text":"  f sim mathcalGPbig(m(cdot) k(cdot cdot)big)","category":"page"},{"location":"examples/gaussian-process-priors/","page":"Gaussian process prior samples","title":"Gaussian process prior samples","text":"In this notebook we show how the choice of kernel affects the samples from a GP (with zero mean).","category":"page"},{"location":"examples/gaussian-process-priors/","page":"Gaussian process prior samples","title":"Gaussian process prior samples","text":"# Load required packages\nusing KernelFunctions, LinearAlgebra\nusing Plots, Plots.PlotMeasures\ndefault(; lw=1.0, legendfontsize=8.0)\nusing Random: seed!\nseed!(42); # reproducibility","category":"page"},{"location":"examples/gaussian-process-priors/#Evaluation-at-finite-set-of-points","page":"Gaussian process prior samples","title":"Evaluation at finite set of points","text":"","category":"section"},{"location":"examples/gaussian-process-priors/","page":"Gaussian process prior samples","title":"Gaussian process prior samples","text":"The function values mathbff = f(x_n)_n=1^N of the GP at a finite number N of points X = x_n_n=1^N follow a multivariate normal distribution mathbff sim mathcalMVN(mathbfm mathrmK) with mean vector mathbfm and covariance matrix mathrmK, where","category":"page"},{"location":"examples/gaussian-process-priors/","page":"Gaussian process prior samples","title":"Gaussian process prior samples","text":"beginaligned\n  mathbfm_i = m(x_i) \n  mathrmK_ij = k(x_i x_j)\nendaligned","category":"page"},{"location":"examples/gaussian-process-priors/","page":"Gaussian process prior samples","title":"Gaussian process prior samples","text":"with 1 le i j le N.","category":"page"},{"location":"examples/gaussian-process-priors/","page":"Gaussian process prior samples","title":"Gaussian process prior samples","text":"We can visualize the infinite-dimensional GP by evaluating it on a fine grid to approximate the dense real line:","category":"page"},{"location":"examples/gaussian-process-priors/","page":"Gaussian process prior samples","title":"Gaussian process prior samples","text":"num_inputs = 101\nxlim = (-5, 5)\nX = range(xlim...; length=num_inputs);","category":"page"},{"location":"examples/gaussian-process-priors/","page":"Gaussian process prior samples","title":"Gaussian process prior samples","text":"Given a kernel k, we can compute the kernel matrix as K = kernelmatrix(k, X).","category":"page"},{"location":"examples/gaussian-process-priors/#Random-samples","page":"Gaussian process prior samples","title":"Random samples","text":"","category":"section"},{"location":"examples/gaussian-process-priors/","page":"Gaussian process prior samples","title":"Gaussian process prior samples","text":"To sample from the multivariate normal distribution p(mathbff) = mathcalMVN(0 mathrmK), we could make use of Distributions.jl and call rand(MvNormal(K)). Alternatively, we could use the AbstractGPs.jl package and construct a GP object which we evaluate at the points of interest and from which we can then sample: rand(GP(k)(X)).","category":"page"},{"location":"examples/gaussian-process-priors/","page":"Gaussian process prior samples","title":"Gaussian process prior samples","text":"Here, we will explicitly construct samples using the Cholesky factorization mathrmL = operatornamecholesky(mathrmK), with mathbff = mathrmL mathbfv, where mathbfv sim mathcalN(0 mathbfI) is a vector of standard-normal random variables.","category":"page"},{"location":"examples/gaussian-process-priors/","page":"Gaussian process prior samples","title":"Gaussian process prior samples","text":"We will use the same randomness mathbfv to generate comparable samples across different kernels.","category":"page"},{"location":"examples/gaussian-process-priors/","page":"Gaussian process prior samples","title":"Gaussian process prior samples","text":"num_samples = 7\nv = randn(num_inputs, num_samples);","category":"page"},{"location":"examples/gaussian-process-priors/","page":"Gaussian process prior samples","title":"Gaussian process prior samples","text":"Mathematically, a kernel matrix is by definition positive semi-definite, but due to finite-precision inaccuracies, the computed kernel matrix might not be exactly positive definite. To avoid Cholesky errors, we add a small \"nugget\" term on the diagonal:","category":"page"},{"location":"examples/gaussian-process-priors/","page":"Gaussian process prior samples","title":"Gaussian process prior samples","text":"function mvn_sample(K)\n    L = cholesky(K + 1e-6 * I)\n    f = L.L * v\n    return f\nend;","category":"page"},{"location":"examples/gaussian-process-priors/#Visualization","page":"Gaussian process prior samples","title":"Visualization","text":"","category":"section"},{"location":"examples/gaussian-process-priors/","page":"Gaussian process prior samples","title":"Gaussian process prior samples","text":"We now define a function that visualizes a kernel for us.","category":"page"},{"location":"examples/gaussian-process-priors/","page":"Gaussian process prior samples","title":"Gaussian process prior samples","text":"function visualize(k::Kernel)\n    K = kernelmatrix(k, X)\n    f = mvn_sample(K)\n\n    p_kernel_2d = heatmap(\n        X,\n        X,\n        K;\n        yflip=true,\n        colorbar=false,\n        ylabel=string(nameof(typeof(k))),\n        ylim=xlim,\n        yticks=([xlim[1], 0, xlim[end]], [\"\\u22125\", raw\"$x'$\", \"5\"]),\n        vlim=(0, 1),\n        title=raw\"$k(x, x')$\",\n        aspect_ratio=:equal,\n        left_margin=5mm,\n    )\n\n    p_kernel_cut = plot(\n        X,\n        k.(X, 0.0);\n        title=string(raw\"$k(x, x_\\mathrm{ref})$\"),\n        label=raw\"$x_\\mathrm{ref}=0.0$\",\n        legend=:topleft,\n        foreground_color_legend=nothing,\n    )\n    plot!(X, k.(X, 1.5); label=raw\"$x_\\mathrm{ref}=1.5$\")\n\n    p_samples = plot(X, f; c=\"blue\", title=raw\"$f(x)$\", ylim=(-3, 3), label=nothing)\n\n    return plot(\n        p_kernel_2d,\n        p_kernel_cut,\n        p_samples;\n        layout=(1, 3),\n        xlabel=raw\"$x$\",\n        xlim=xlim,\n        xticks=collect(xlim),\n    )\nend;","category":"page"},{"location":"examples/gaussian-process-priors/","page":"Gaussian process prior samples","title":"Gaussian process prior samples","text":"We can now visualize a kernel and show samples from a Gaussian process with a given kernel:","category":"page"},{"location":"examples/gaussian-process-priors/","page":"Gaussian process prior samples","title":"Gaussian process prior samples","text":"plot(visualize(SqExponentialKernel()); size=(800, 210), bottommargin=5mm, topmargin=5mm)","category":"page"},{"location":"examples/gaussian-process-priors/","page":"Gaussian process prior samples","title":"Gaussian process prior samples","text":"(Image: )","category":"page"},{"location":"examples/gaussian-process-priors/#Kernel-comparison","page":"Gaussian process prior samples","title":"Kernel comparison","text":"","category":"section"},{"location":"examples/gaussian-process-priors/","page":"Gaussian process prior samples","title":"Gaussian process prior samples","text":"This also allows us to compare different kernels:","category":"page"},{"location":"examples/gaussian-process-priors/","page":"Gaussian process prior samples","title":"Gaussian process prior samples","text":"kernels = [\n    Matern12Kernel(),\n    Matern32Kernel(),\n    Matern52Kernel(),\n    SqExponentialKernel(),\n    WhiteKernel(),\n    ConstantKernel(),\n    LinearKernel(),\n    compose(PeriodicKernel(), ScaleTransform(0.2)),\n    NeuralNetworkKernel(),\n]\nplot(\n    [visualize(k) for k in kernels]...;\n    layout=(length(kernels), 1),\n    size=(800, 220 * length(kernels) + 100),\n)","category":"page"},{"location":"examples/gaussian-process-priors/","page":"Gaussian process prior samples","title":"Gaussian process prior samples","text":"(Image: )","category":"page"},{"location":"examples/gaussian-process-priors/","page":"Gaussian process prior samples","title":"Gaussian process prior samples","text":"","category":"page"},{"location":"examples/gaussian-process-priors/","page":"Gaussian process prior samples","title":"Gaussian process prior samples","text":"This page was generated using Literate.jl.","category":"page"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"  CurrentModule = KernelFunctions","category":"page"},{"location":"kernels/#Kernel-Functions","page":"Kernel Functions","title":"Kernel Functions","text":"","category":"section"},{"location":"kernels/#base_kernels","page":"Kernel Functions","title":"Base Kernels","text":"","category":"section"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"These are the basic kernels without any transformation of the data. They are the building blocks of KernelFunctions.","category":"page"},{"location":"kernels/#Constant-Kernels","page":"Kernel Functions","title":"Constant Kernels","text":"","category":"section"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"ZeroKernel\nConstantKernel\nWhiteKernel\nEyeKernel","category":"page"},{"location":"kernels/#KernelFunctions.ZeroKernel","page":"Kernel Functions","title":"KernelFunctions.ZeroKernel","text":"ZeroKernel()\n\nZero kernel.\n\nDefinition\n\nFor inputs x x, the zero kernel is defined as\n\nk(x x) = 0\n\nThe output type depends on x and x.\n\nSee also: ConstantKernel\n\n\n\n\n\n","category":"type"},{"location":"kernels/#KernelFunctions.ConstantKernel","page":"Kernel Functions","title":"KernelFunctions.ConstantKernel","text":"ConstantKernel(; c::Real=1.0)\n\nKernel of constant value c.\n\nDefinition\n\nFor inputs x x, the kernel of constant value c geq 0 is defined as\n\nk(x x) = c\n\nSee also: ZeroKernel\n\n\n\n\n\n","category":"type"},{"location":"kernels/#KernelFunctions.WhiteKernel","page":"Kernel Functions","title":"KernelFunctions.WhiteKernel","text":"WhiteKernel()\n\nWhite noise kernel.\n\nDefinition\n\nFor inputs x x, the white noise kernel is defined as\n\nk(x x) = delta(x x)\n\n\n\n\n\n","category":"type"},{"location":"kernels/#KernelFunctions.EyeKernel","page":"Kernel Functions","title":"KernelFunctions.EyeKernel","text":"EyeKernel()\n\nAlias of WhiteKernel.\n\n\n\n\n\n","category":"type"},{"location":"kernels/#Cosine-Kernel","page":"Kernel Functions","title":"Cosine Kernel","text":"","category":"section"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"CosineKernel","category":"page"},{"location":"kernels/#KernelFunctions.CosineKernel","page":"Kernel Functions","title":"KernelFunctions.CosineKernel","text":"CosineKernel(; metric=Euclidean())\n\nCosine kernel with respect to the metric.\n\nDefinition\n\nFor inputs x x and metric d(cdot cdot), the cosine kernel is defined as\n\nk(x x) = cos(pi d(x x))\n\nBy default, d is the Euclidean metric d(x x) = x - x_2.\n\n\n\n\n\n","category":"type"},{"location":"kernels/#Exponential-Kernels","page":"Kernel Functions","title":"Exponential Kernels","text":"","category":"section"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"ExponentialKernel\nLaplacianKernel\nSqExponentialKernel\nSEKernel\nGaussianKernel\nRBFKernel\nGammaExponentialKernel","category":"page"},{"location":"kernels/#KernelFunctions.ExponentialKernel","page":"Kernel Functions","title":"KernelFunctions.ExponentialKernel","text":"ExponentialKernel(; metric=Euclidean())\n\nExponential kernel with respect to the metric.\n\nDefinition\n\nFor inputs x x and metric d(cdot cdot), the exponential kernel is defined as\n\nk(x x) = expbig(- d(x x)big)\n\nBy default, d is the Euclidean metric d(x x) = x - x_2.\n\nSee also: GammaExponentialKernel\n\n\n\n\n\n","category":"type"},{"location":"kernels/#KernelFunctions.LaplacianKernel","page":"Kernel Functions","title":"KernelFunctions.LaplacianKernel","text":"LaplacianKernel()\n\nAlias of ExponentialKernel.\n\n\n\n\n\n","category":"type"},{"location":"kernels/#KernelFunctions.SqExponentialKernel","page":"Kernel Functions","title":"KernelFunctions.SqExponentialKernel","text":"SqExponentialKernel(; metric=Euclidean())\n\nSquared exponential kernel with respect to the metric.\n\nDefinition\n\nFor inputs x x and metric d(cdot cdot), the squared exponential kernel is defined as\n\nk(x x) = expbigg(- fracd(x x)^22bigg)\n\nBy default, d is the Euclidean metric d(x x) = x - x_2.\n\nSee also: GammaExponentialKernel\n\n\n\n\n\n","category":"type"},{"location":"kernels/#KernelFunctions.SEKernel","page":"Kernel Functions","title":"KernelFunctions.SEKernel","text":"SEKernel()\n\nAlias of SqExponentialKernel.\n\n\n\n\n\n","category":"type"},{"location":"kernels/#KernelFunctions.GaussianKernel","page":"Kernel Functions","title":"KernelFunctions.GaussianKernel","text":"GaussianKernel()\n\nAlias of SqExponentialKernel.\n\n\n\n\n\n","category":"type"},{"location":"kernels/#KernelFunctions.RBFKernel","page":"Kernel Functions","title":"KernelFunctions.RBFKernel","text":"RBFKernel()\n\nAlias of SqExponentialKernel.\n\n\n\n\n\n","category":"type"},{"location":"kernels/#KernelFunctions.GammaExponentialKernel","page":"Kernel Functions","title":"KernelFunctions.GammaExponentialKernel","text":"GammaExponentialKernel(; γ::Real=1.0, metric=Euclidean())\n\nγ-exponential kernel with respect to the metric and with parameter γ.\n\nDefinition\n\nFor inputs x x and metric d(cdot cdot), the γ-exponential kernel[RW] with parameter gamma in (0 2 is defined as\n\nk(x x gamma) = expbig(- d(x x)^gammabig)\n\nBy default, d is the Euclidean metric d(x x) = x - x_2.\n\nSee also: ExponentialKernel, SqExponentialKernel\n\n[RW]: C. E. Rasmussen & C. K. I. Williams (2006). Gaussian Processes for Machine Learning.\n\n\n\n\n\n","category":"type"},{"location":"kernels/#Exponentiated-Kernel","page":"Kernel Functions","title":"Exponentiated Kernel","text":"","category":"section"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"ExponentiatedKernel","category":"page"},{"location":"kernels/#KernelFunctions.ExponentiatedKernel","page":"Kernel Functions","title":"KernelFunctions.ExponentiatedKernel","text":"ExponentiatedKernel()\n\nExponentiated kernel.\n\nDefinition\n\nFor inputs x x in mathbbR^d, the exponentiated kernel is defined as\n\nk(x x) = exp(x^top x)\n\n\n\n\n\n","category":"type"},{"location":"kernels/#Fractional-Brownian-Motion-Kernel","page":"Kernel Functions","title":"Fractional Brownian Motion Kernel","text":"","category":"section"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"FBMKernel","category":"page"},{"location":"kernels/#KernelFunctions.FBMKernel","page":"Kernel Functions","title":"KernelFunctions.FBMKernel","text":"FBMKernel(; h::Real=0.5)\n\nFractional Brownian motion kernel with Hurst index h.\n\nDefinition\n\nFor inputs x x in mathbbR^d, the fractional Brownian motion kernel with Hurst index h in 01 is defined as\n\nk(x x h) =  fracx_2^2h + x_2^2h - x - x^2h2\n\n\n\n\n\n","category":"type"},{"location":"kernels/#Gabor-Kernel","page":"Kernel Functions","title":"Gabor Kernel","text":"","category":"section"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"gaborkernel","category":"page"},{"location":"kernels/#KernelFunctions.gaborkernel","page":"Kernel Functions","title":"KernelFunctions.gaborkernel","text":"gaborkernel(;\n    sqexponential_transform=IdentityTransform(), cosine_tranform=IdentityTransform()\n)\n\nConstruct a Gabor kernel with transformations sqexponential_transform and cosine_transform of the inputs of the underlying squared exponential and cosine kernel, respectively.\n\nDefinition\n\nFor inputs x x in mathbbR^d, the Gabor kernel with transformations f and g of the inputs to the squared exponential and cosine kernel, respectively, is defined as\n\nk(x x f g) = expbigg(- frac f(x) - f(x)_2^22bigg)\n                 cosbig(pi g(x) - g(x)_2 big)\n\n\n\n\n\n","category":"function"},{"location":"kernels/#Matérn-Kernels","page":"Kernel Functions","title":"Matérn Kernels","text":"","category":"section"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"MaternKernel\nMatern12Kernel\nMatern32Kernel\nMatern52Kernel","category":"page"},{"location":"kernels/#KernelFunctions.MaternKernel","page":"Kernel Functions","title":"KernelFunctions.MaternKernel","text":"MaternKernel(; ν::Real=1.5, metric=Euclidean())\n\nMatérn kernel of order ν with respect to the metric.\n\nDefinition\n\nFor inputs x x and metric d(cdot cdot), the Matérn kernel of order nu  0 is defined as\n\nk(xxnu) = frac2^1-nuGamma(nu)big(sqrt2nu d(x x)big) K_nubig(sqrt2nu d(x x)big)\n\nwhere Gamma is the Gamma function and K_nu is the modified Bessel function of the second kind of order nu. By default, d is the Euclidean metric d(x x) = x - x_2.\n\nA Gaussian process with a Matérn kernel is lceil nu rceil - 1-times differentiable in the mean-square sense.\n\nSee also: Matern12Kernel, Matern32Kernel, Matern52Kernel\n\n\n\n\n\n","category":"type"},{"location":"kernels/#KernelFunctions.Matern12Kernel","page":"Kernel Functions","title":"KernelFunctions.Matern12Kernel","text":"Matern12Kernel()\n\nAlias of ExponentialKernel.\n\n\n\n\n\n","category":"type"},{"location":"kernels/#KernelFunctions.Matern32Kernel","page":"Kernel Functions","title":"KernelFunctions.Matern32Kernel","text":"Matern32Kernel(; metric=Euclidean())\n\nMatérn kernel of order 32 with respect to the metric.\n\nDefinition\n\nFor inputs x x and metric d(cdot cdot), the Matérn kernel of order 32 is  given by\n\nk(x x) = big(1 + sqrt3 d(x x) big) expbig(- sqrt3 d(x x) big)\n\nBy default, d is the Euclidean metric d(x x) = x - x_2.\n\nSee also: MaternKernel\n\n\n\n\n\n","category":"type"},{"location":"kernels/#KernelFunctions.Matern52Kernel","page":"Kernel Functions","title":"KernelFunctions.Matern52Kernel","text":"Matern52Kernel(; metric=Euclidean())\n\nMatérn kernel of order 52 with respect to the metric.\n\nDefinition\n\nFor inputs x x and metric d(cdot cdot), the Matérn kernel of order 52 is given by\n\nk(x x) = bigg(1 + sqrt5 d(x x) + frac53 d(x x)^2bigg)\n           expbig(- sqrt5 d(x x) big)\n\nBy default, d is the Euclidean metric d(x x) = x - x_2.\n\nSee also: MaternKernel\n\n\n\n\n\n","category":"type"},{"location":"kernels/#Neural-Network-Kernel","page":"Kernel Functions","title":"Neural Network Kernel","text":"","category":"section"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"NeuralNetworkKernel","category":"page"},{"location":"kernels/#KernelFunctions.NeuralNetworkKernel","page":"Kernel Functions","title":"KernelFunctions.NeuralNetworkKernel","text":"NeuralNetworkKernel()\n\nKernel of a Gaussian process obtained as the limit of a Bayesian neural network with a single hidden layer as the number of units goes to infinity.\n\nDefinition\n\nConsider the single-layer Bayesian neural network f colon mathbbR^d to mathbbR with h hidden units defined by\n\nf(x b v u) = b + sqrtfracpi2 sum_i=1^h v_i mathrmerfbig(u_i^top xbig)\n\nwhere mathrmerf is the error function, and with prior distributions\n\nbeginaligned\nb sim mathcalN(0 sigma_b^2)\nv sim mathcalN(0 sigma_v^2 mathrmI_hh)\nu_i sim mathcalN(0 mathrmI_d2) qquad (i = 1ldotsh)\nendaligned\n\nAs h to infty, the neural network converges to the Gaussian process\n\ng(cdot) sim mathcalGPbig(0 sigma_b^2 + sigma_v^2 k(cdot cdot)big)\n\nwhere the neural network kernel k is given by\n\nk(x x) = arcsinleft(fracx^top xsqrtbig(1 + x^2_2big) big(1 + x_2^2big)right)\n\nfor inputs x x in mathbbR^d.[CW]\n\n[CW]: C. K. I. Williams (1998). Computation with infinite neural networks.\n\n\n\n\n\n","category":"type"},{"location":"kernels/#Periodic-Kernel","page":"Kernel Functions","title":"Periodic Kernel","text":"","category":"section"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"PeriodicKernel\nPeriodicKernel(::DataType, ::Int)","category":"page"},{"location":"kernels/#KernelFunctions.PeriodicKernel","page":"Kernel Functions","title":"KernelFunctions.PeriodicKernel","text":"PeriodicKernel(; r::AbstractVector=ones(Float64, 1))\n\nPeriodic kernel with parameter r.\n\nDefinition\n\nFor inputs x x in mathbbR^d, the periodic kernel with parameter r_i  0 is defined[DM] as\n\nk(x x r) = expbigg(- frac12 sum_i=1^d bigg(fracsinbig(pi(x_i - x_i)big)r_ibigg)^2bigg)\n\n[DM]: D. J. C. MacKay (1998). Introduction to Gaussian Processes.\n\n\n\n\n\n","category":"type"},{"location":"kernels/#KernelFunctions.PeriodicKernel-Tuple{DataType, Int64}","page":"Kernel Functions","title":"KernelFunctions.PeriodicKernel","text":"PeriodicKernel([T=Float64, dims::Int=1])\n\nCreate a PeriodicKernel with parameter r=ones(T, dims).\n\n\n\n\n\n","category":"method"},{"location":"kernels/#Piecewise-Polynomial-Kernel","page":"Kernel Functions","title":"Piecewise Polynomial Kernel","text":"","category":"section"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"PiecewisePolynomialKernel","category":"page"},{"location":"kernels/#KernelFunctions.PiecewisePolynomialKernel","page":"Kernel Functions","title":"KernelFunctions.PiecewisePolynomialKernel","text":"PiecewisePolynomialKernel(; dim::Int, degree::Int=0, metric=Euclidean())\nPiecewisePolynomialKernel{degree}(; dim::Int, metric=Euclidean())\n\nPiecewise polynomial kernel of degree degree for inputs of dimension dim with support in the unit ball with respect to the metric.\n\nDefinition\n\nFor inputs x x of dimension m and metric d(cdot cdot), the piecewise polynomial kernel of degree v in 0123 is defined as\n\nk(x x v) = max(1 - d(x x) 0)^alpha(vm) f_vm(d(x x))\n\nwhere alpha(v m) = lfloor fracm2rfloor + 2v + 1 and f_vm are polynomials of degree v given by\n\nbeginaligned\nf_0m(r) = 1 \nf_1m(r) = 1 + (j + 1) r \nf_2m(r) = 1 + (j + 2) r + big((j^2 + 4j + 3)  3big) r^2 \nf_3m(r) = 1 + (j + 3) r + big((6 j^2 + 36j + 45)  15big) r^2 + big((j^3 + 9 j^2 + 23j + 15)  15big) r^3\nendaligned\n\nwhere j = lfloor fracm2rfloor + v + 1. By default, d is the Euclidean metric d(x x) = x - x_2.\n\nThe kernel is 2v times continuously differentiable and the corresponding Gaussian process is hence v times mean-square differentiable.\n\n\n\n\n\n","category":"type"},{"location":"kernels/#Polynomial-Kernels","page":"Kernel Functions","title":"Polynomial Kernels","text":"","category":"section"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"LinearKernel\nPolynomialKernel","category":"page"},{"location":"kernels/#KernelFunctions.LinearKernel","page":"Kernel Functions","title":"KernelFunctions.LinearKernel","text":"LinearKernel(; c::Real=0.0)\n\nLinear kernel with constant offset c.\n\nDefinition\n\nFor inputs x x in mathbbR^d, the linear kernel with constant offset c geq 0 is defined as\n\nk(x x c) = x^top x + c\n\nSee also: PolynomialKernel\n\n\n\n\n\n","category":"type"},{"location":"kernels/#KernelFunctions.PolynomialKernel","page":"Kernel Functions","title":"KernelFunctions.PolynomialKernel","text":"PolynomialKernel(; degree::Int=2, c::Real=0.0)\n\nPolynomial kernel of degree degree with constant offset c.\n\nDefinition\n\nFor inputs x x in mathbbR^d, the polynomial kernel of degree nu in mathbbN with constant offset c geq 0 is defined as\n\nk(x x c nu) = (x^top x + c)^nu\n\nSee also: LinearKernel\n\n\n\n\n\n","category":"type"},{"location":"kernels/#Rational-Kernels","page":"Kernel Functions","title":"Rational Kernels","text":"","category":"section"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"RationalKernel\nRationalQuadraticKernel\nGammaRationalKernel","category":"page"},{"location":"kernels/#KernelFunctions.RationalKernel","page":"Kernel Functions","title":"KernelFunctions.RationalKernel","text":"RationalKernel(; α::Real=2.0, metric=Euclidean())\n\nRational kernel with shape parameter α and given metric.\n\nDefinition\n\nFor inputs x x, the rational kernel with shape parameter alpha  0 is defined as\n\nk(x x alpha) = bigg(1 + fracx - xalphabigg)^-alpha\n\nThe ExponentialKernel is recovered in the limit as alpha to infty.\n\nSee also: GammaRationalKernel\n\n\n\n\n\n","category":"type"},{"location":"kernels/#KernelFunctions.RationalQuadraticKernel","page":"Kernel Functions","title":"KernelFunctions.RationalQuadraticKernel","text":"RationalQuadraticKernel(; α::Real=2.0, metric=Euclidean())\n\nRational-quadratic kernel with respect to the metric and with shape parameter α.\n\nDefinition\n\nFor inputs x x and metric d(cdot cdot), the rational-quadratic kernel with shape parameter alpha  0 is defined as\n\nk(x x alpha) = bigg(1 + fracd(x x)^22alphabigg)^-alpha\n\nBy default, d is the Euclidean metric d(x x) = x - x_2.\n\nThe SqExponentialKernel is recovered in the limit as alpha to infty.\n\nSee also: GammaRationalKernel\n\n\n\n\n\n","category":"type"},{"location":"kernels/#KernelFunctions.GammaRationalKernel","page":"Kernel Functions","title":"KernelFunctions.GammaRationalKernel","text":"GammaRationalKernel(; α::Real=2.0, γ::Real=1.0, metric=Euclidean())\n\nγ-rational kernel with respect to the metric with shape parameters α and γ.\n\nDefinition\n\nFor inputs x x and metric d(cdot cdot), the γ-rational kernel with shape parameters alpha  0 and gamma in (0 2 is defined as\n\nk(x x alpha gamma) = bigg(1 + fracd(x x)^gammaalphabigg)^-alpha\n\nBy default, d is the Euclidean metric d(x x) = x - x_2.\n\nThe GammaExponentialKernel is recovered in the limit as alpha to infty.\n\nSee also: RationalKernel, RationalQuadraticKernel\n\n\n\n\n\n","category":"type"},{"location":"kernels/#Spectral-Mixture-Kernels","page":"Kernel Functions","title":"Spectral Mixture Kernels","text":"","category":"section"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"spectral_mixture_kernel\nspectral_mixture_product_kernel","category":"page"},{"location":"kernels/#KernelFunctions.spectral_mixture_kernel","page":"Kernel Functions","title":"KernelFunctions.spectral_mixture_kernel","text":"spectral_mixture_kernel(\n    h::Kernel=SqExponentialKernel(),\n    αs::AbstractVector{<:Real},\n    γs::AbstractMatrix{<:Real},\n    ωs::AbstractMatrix{<:Real},\n)\n\nwhere αs are the weights of dimension (A, ), γs is the covariance matrix of dimension (D, A) and ωs are the mean vectors and is of dimension (D, A). Here, D is input dimension and A is the number of spectral components.\n\nh is the kernel, which defaults to SqExponentialKernel if not specified.\n\nGeneralised Spectral Mixture kernel function. This family of functions is  dense in the family of stationary real-valued kernels with respect to the pointwise convergence.[1]\n\n   κ(x y) = αs (h(-(γs * t)^2) * cos(π * ωs * t) t = x - y\n\nReferences:\n\n[1] Generalized Spectral Kernels, by Yves-Laurent Kom Samo and Stephen J. Roberts\n[2] SM: Gaussian Process Kernels for Pattern Discovery and Extrapolation,\n        ICML, 2013, by Andrew Gordon Wilson and Ryan Prescott Adams,\n[3] Covariance kernels for fast automatic pattern discovery and extrapolation\n    with Gaussian processes, Andrew Gordon Wilson, PhD Thesis, January 2014.\n    http://www.cs.cmu.edu/~andrewgw/andrewgwthesis.pdf\n[4] http://www.cs.cmu.edu/~andrewgw/pattern/.\n\n\n\n\n\n","category":"function"},{"location":"kernels/#KernelFunctions.spectral_mixture_product_kernel","page":"Kernel Functions","title":"KernelFunctions.spectral_mixture_product_kernel","text":"spectral_mixture_product_kernel(\n    h::Kernel=SqExponentialKernel(),\n    αs::AbstractMatrix{<:Real},\n    γs::AbstractMatrix{<:Real},\n    ωs::AbstractMatrix{<:Real},\n)\n\nwhere αs are the weights of dimension (D, A), γs is the covariance matrix of dimension (D, A) and ωs are the mean vectors and is of dimension (D, A). Here, D is input dimension and A is the number of spectral components.\n\nSpectral Mixture Product Kernel. With enough components A, the SMP kernel can model any product kernel to arbitrary precision, and is flexible even with a small number of components [1]\n\nh is the kernel, which defaults to SqExponentialKernel if not specified.\n\n   κ(x y) = Πᵢ₁ᴷ Σ(αsᵢᵀ * (h(-(γsᵢᵀ * tᵢ)²) * cos(ωsᵢᵀ * tᵢ))) tᵢ = xᵢ - yᵢ\n\nReferences:\n\n[1] GPatt: Fast Multidimensional Pattern Extrapolation with GPs,\n    arXiv 1310.5288, 2013, by Andrew Gordon Wilson, Elad Gilboa,\n    Arye Nehorai and John P. Cunningham\n\n\n\n\n\n","category":"function"},{"location":"kernels/#Wiener-Kernel","page":"Kernel Functions","title":"Wiener Kernel","text":"","category":"section"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"WienerKernel","category":"page"},{"location":"kernels/#KernelFunctions.WienerKernel","page":"Kernel Functions","title":"KernelFunctions.WienerKernel","text":"WienerKernel(; i::Int=0)\nWienerKernel{i}()\n\nThe i-times integrated Wiener process kernel function.\n\nDefinition\n\nFor inputs x x in mathbbR^d, the i-times integrated Wiener process kernel with i in -1 0 1 2 3 is defined[SDH] as\n\nk_i(x x) = begincases\n    delta(x x)  textif  i=-1\n    minbig(x_2 x_2big)  textif  i=0\n    a_i1^-1 minbig(x_2 x_2big)^2i + 1\n    + a_i2^-1 x - x_2 r_ibig(x_2 x_2big) minbig(x_2 x_2big)^i + 1\n     textotherwise\nendcases\n\nwhere the coefficients a are given by\n\na = beginbmatrix\n3  2 \n20  12 \n252  720\nendbmatrix\n\nand the functions r_i are defined as\n\nbeginaligned\nr_1(t t) = 1\nr_2(t t) = t + t - fracmin(t t)2\nr_3(t t) = 5 max(t t)^2 + 2 tt + 3 min(t t)^2\nendaligned\n\nThe WhiteKernel is recovered for i = -1.\n\n[SDH]: Schober, Duvenaud & Hennig (2014). Probabilistic ODE Solvers with Runge-Kutta Means.\n\n\n\n\n\n","category":"type"},{"location":"kernels/#Composite-Kernels","page":"Kernel Functions","title":"Composite Kernels","text":"","category":"section"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"The modular design of KernelFunctions uses base kernels as building blocks for more complex kernels. There are a variety of composite kernels implemented, including those which transform the inputs to a wrapped kernel to implement length scales, scale the variance of a kernel, and sum or multiply collections of kernels together.","category":"page"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"TransformedKernel\n∘(::Kernel, ::Transform)\nScaledKernel\nKernelSum\nKernelProduct\nKernelTensorProduct\nNormalizedKernel","category":"page"},{"location":"kernels/#KernelFunctions.TransformedKernel","page":"Kernel Functions","title":"KernelFunctions.TransformedKernel","text":"TransformedKernel(k::Kernel, t::Transform)\n\nKernel derived from k for which inputs are transformed via a Transform t.\n\nThe preferred way to create kernels with input transformations is to use the composition operator ∘ or its alias compose instead of TransformedKernel directly since this allows optimized implementations for specific kernels and transformations.\n\nSee also: ∘\n\n\n\n\n\n","category":"type"},{"location":"kernels/#Base.:∘-Tuple{Kernel, Transform}","page":"Kernel Functions","title":"Base.:∘","text":"kernel ∘ transform\n∘(kernel, transform)\ncompose(kernel, transform)\n\nCompose a kernel with a transformation transform of its inputs.\n\nThe prefix forms support chains of multiple transformations: ∘(kernel, transform1, transform2) = kernel ∘ transform1 ∘ transform2.\n\nDefinition\n\nFor inputs x x, the transformed kernel widetildek derived from kernel k by input transformation t is defined as\n\nwidetildek(x x k t) = kbig(t(x) t(x)big)\n\nExamples\n\njulia> (SqExponentialKernel() ∘ ScaleTransform(0.5))(0, 2) == exp(-0.5)\ntrue\n\njulia> ∘(ExponentialKernel(), ScaleTransform(2), ScaleTransform(0.5))(1, 2) == exp(-1)\ntrue\n\nSee also: TransformedKernel\n\n\n\n\n\n","category":"method"},{"location":"kernels/#KernelFunctions.ScaledKernel","page":"Kernel Functions","title":"KernelFunctions.ScaledKernel","text":"ScaledKernel(k::Kernel, σ²::Real=1.0)\n\nScaled kernel derived from k by multiplication with variance σ².\n\nDefinition\n\nFor inputs x x, the scaled kernel widetildek derived from kernel k by multiplication with variance sigma^2  0 is defined as\n\nwidetildek(x x k sigma^2) = sigma^2 k(x x)\n\n\n\n\n\n","category":"type"},{"location":"kernels/#KernelFunctions.KernelSum","page":"Kernel Functions","title":"KernelFunctions.KernelSum","text":"KernelSum <: Kernel\n\nCreate a sum of kernels. One can also use the operator +.\n\nThere are various ways in which you create a KernelSum:\n\nThe simplest way to specify a KernelSum would be to use the overloaded + operator. This is  equivalent to creating a KernelSum by specifying the kernels as the arguments to the constructor.  \n\njulia> k1 = SqExponentialKernel(); k2 = LinearKernel(); X = rand(5);\n\njulia> (k = k1 + k2) == KernelSum(k1, k2)\ntrue\n\njulia> kernelmatrix(k1 + k2, X) == kernelmatrix(k1, X) .+ kernelmatrix(k2, X)\ntrue\n\njulia> kernelmatrix(k, X) == kernelmatrix(k1 + k2, X)\ntrue\n\nYou could also specify a KernelSum by providing a Tuple or a Vector of the  kernels to be summed. We suggest you to use a Tuple when you have fewer components   and a Vector when dealing with a large number of components.\n\njulia> KernelSum((k1, k2)) == k1 + k2\ntrue\n\njulia> KernelSum([k1, k2]) == KernelSum((k1, k2)) == k1 + k2\ntrue\n\n\n\n\n\n","category":"type"},{"location":"kernels/#KernelFunctions.KernelProduct","page":"Kernel Functions","title":"KernelFunctions.KernelProduct","text":"KernelProduct <: Kernel\n\nCreate a product of kernels. One can also use the overloaded operator *.\n\nThere are various ways in which you create a KernelProduct:\n\nThe simplest way to specify a KernelProduct would be to use the overloaded * operator. This is  equivalent to creating a KernelProduct by specifying the kernels as the arguments to the constructor.  \n\njulia> k1 = SqExponentialKernel(); k2 = LinearKernel(); X = rand(5);\n\njulia> (k = k1 * k2) == KernelProduct(k1, k2)\ntrue\n\njulia> kernelmatrix(k1 * k2, X) == kernelmatrix(k1, X) .* kernelmatrix(k2, X)\ntrue\n\njulia> kernelmatrix(k, X) == kernelmatrix(k1 * k2, X)\ntrue\n\nYou could also specify a KernelProduct by providing a Tuple or a Vector of the  kernels to be multiplied. We suggest you to use a Tuple when you have fewer components   and a Vector when dealing with a large number of components.\n\njulia> KernelProduct((k1, k2)) == k1 * k2\ntrue\n\njulia> KernelProduct([k1, k2]) == KernelProduct((k1, k2)) == k1 * k2\ntrue\n\n\n\n\n\n","category":"type"},{"location":"kernels/#KernelFunctions.KernelTensorProduct","page":"Kernel Functions","title":"KernelFunctions.KernelTensorProduct","text":"KernelTensorProduct\n\nTensor product of kernels.\n\nDefinition\n\nFor inputs x = (x_1 ldots x_n) and x = (x_1 ldots x_n), the tensor product of kernels k_1 ldots k_n is defined as\n\nk(x x k_1 ldots k_n) = Big(bigotimes_i=1^n k_iBig)(x x) = prod_i=1^n k_i(x_i x_i)\n\nConstruction\n\nThe simplest way to specify a KernelTensorProduct is to use the overloaded tensor operator or its alias ⊗ (can be typed by \\otimes<tab>).\n\njulia> k1 = SqExponentialKernel(); k2 = LinearKernel(); X = rand(5, 2);\n\njulia> kernelmatrix(k1 ⊗ k2, RowVecs(X)) == kernelmatrix(k1, X[:, 1]) .* kernelmatrix(k2, X[:, 2])\ntrue\n\nYou can also specify a KernelTensorProduct by providing kernels as individual arguments or as an iterable data structure such as a Tuple or a Vector. Using a tuple or individual arguments guarantees that KernelTensorProduct is concretely typed but might lead to large compilation times if the number of kernels is large.\n\njulia> KernelTensorProduct(k1, k2) == k1 ⊗ k2\ntrue\n\njulia> KernelTensorProduct((k1, k2)) == k1 ⊗ k2\ntrue\n\njulia> KernelTensorProduct([k1, k2]) == k1 ⊗ k2\ntrue\n\n\n\n\n\n","category":"type"},{"location":"kernels/#KernelFunctions.NormalizedKernel","page":"Kernel Functions","title":"KernelFunctions.NormalizedKernel","text":"NormalizedKernel(k::Kernel)\n\nA normalized kernel derived from k.\n\nDefinition\n\nFor inputs x x, the normalized kernel widetildek derived from kernel k is defined as\n\nwidetildek(x x k) = frack(x x)sqrtk(x x) k(x x)\n\n\n\n\n\n","category":"type"},{"location":"kernels/#Multi-output-Kernels","page":"Kernel Functions","title":"Multi-output Kernels","text":"","category":"section"},{"location":"kernels/","page":"Kernel Functions","title":"Kernel Functions","text":"MOKernel\nIndependentMOKernel\nLatentFactorMOKernel\nIntrinsicCoregionMOKernel\nLinearMixingModelKernel","category":"page"},{"location":"kernels/#KernelFunctions.MOKernel","page":"Kernel Functions","title":"KernelFunctions.MOKernel","text":"MOKernel\n\nAbstract type for kernels with multiple outpus.\n\n\n\n\n\n","category":"type"},{"location":"kernels/#KernelFunctions.IndependentMOKernel","page":"Kernel Functions","title":"KernelFunctions.IndependentMOKernel","text":"IndependentMOKernel(k::Kernel)\n\nKernel for multiple independent outputs with kernel k each.\n\nDefinition\n\nFor inputs x x and output dimensions p_x p_x, the kernel widetildek for independent outputs with kernel k each is defined as\n\nwidetildekbig((x p_x) (x p_x)big) = begincases\n    k(x x)  textif  p_x = p_x \n    0  textotherwise\nendcases\n\nMathematically, it is equivalent to a matrix-valued kernel defined as\n\nwidetildeK(x x) = mathrmdiagbig(k(x x) ldots k(x x)big) in mathbbR^m times m\n\nwhere m is the number of outputs.\n\n\n\n\n\n","category":"type"},{"location":"kernels/#KernelFunctions.LatentFactorMOKernel","page":"Kernel Functions","title":"KernelFunctions.LatentFactorMOKernel","text":"LatentFactorMOKernel(g, e::MOKernel, A::AbstractMatrix)\n\nKernel associated with the semiparametric latent factor model.\n\nDefinition\n\nFor inputs x x and output dimensions p_x p_x, the kernel is defined as[STJ]\n\nkbig((x p_x) (x p_x)big) = sum^Q_q=1 A_p_xqg_q(x x)A_p_xq\n                                   + ebig((x p_x) (x p_x)big)\n\nwhere g_1 ldots g_Q are Q kernels, one for each latent process, e is a multi-output kernel for m outputs, and A is a matrix of weights for the kernels of size m times Q.\n\n[STJ]: M. Seeger, Y. Teh, & M. I. Jordan (2005). Semiparametric Latent Factor Models.\n\n\n\n\n\n","category":"type"},{"location":"kernels/#KernelFunctions.IntrinsicCoregionMOKernel","page":"Kernel Functions","title":"KernelFunctions.IntrinsicCoregionMOKernel","text":"IntrinsicCoregionMOKernel(; kernel::Kernel, B::AbstractMatrix)\n\nKernel associated with the intrinsic coregionalization model.\n\nDefinition\n\nFor inputs x x and output dimensions p p, the kernel is defined as[ARL]\n\nkbig((x p) (x p) B tildekbig) = B_p p tildekbig(x xbig)\n\nwhere B is a positive semidefinite matrix of size m times m, with m being the number of outputs, and tildek is a scalar-valued kernel shared by the latent processes.\n\n[ARL]: M. Álvarez, L. Rosasco, & N. Lawrence (2012). Kernels for Vector-Valued Functions: a Review.\n\n\n\n\n\n","category":"type"},{"location":"kernels/#KernelFunctions.LinearMixingModelKernel","page":"Kernel Functions","title":"KernelFunctions.LinearMixingModelKernel","text":"LinearMixingModelKernel(g, e::MOKernel, A::AbstractMatrix)\n\nKernel associated with the linear mixing model.\n\nDefinition\n\nFor inputs x x and output dimensions p_x p_x, the kernel is defined as[BPTHST]\n\nkbig((x p_x) (x p_x)big) = H_p_xK(x x)H_p_x\n\nwhere K(x x) = Diag(k_1(x x)  k_m(x x)) with zero off-diagonal entries. H_p_x is the p_x-th column (p_x-th output) of H in mathbbR^m times p representing m basis vectors for the p dimensional output space of f. k_1 ldots k_m are m kernels, one for each latent process, H is a mixing matrix of m basis vectors spanning the output space.\n\n[BPTHST]: Wessel P. Bruinsma, Eric Perim, Will Tebbutt, J. Scott Hosking, Arno Solin, Richard E. Turner (2020). Scalable Exact Inference in Multi-Output Gaussian Processes.\n\n\n\n\n\n","category":"type"},{"location":"api/#API-Library","page":"API","title":"API Library","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"CurrentModule = KernelFunctions","category":"page"},{"location":"api/#Functions","page":"API","title":"Functions","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"The KernelFunctions API comprises the following four functions.","category":"page"},{"location":"api/","page":"API","title":"API","text":"kernelmatrix\nkernelmatrix!\nkernelmatrix_diag\nkernelmatrix_diag!","category":"page"},{"location":"api/#KernelFunctions.kernelmatrix","page":"API","title":"KernelFunctions.kernelmatrix","text":"kernelmatrix(κ::Kernel, x::AbstractVector)\n\nCompute the kernel κ for each pair of inputs in x. Returns a matrix of size (length(x), length(x)) satisfying kernelmatrix(κ, x)[p, q] == κ(x[p], x[q]).\n\nkernelmatrix(κ::Kernel, x::AbstractVector, y::AbstractVector)\n\nCompute the kernel κ for each pair of inputs in x and y. Returns a matrix of size (length(x), length(y)) satisfying kernelmatrix(κ, x, y)[p, q] == κ(x[p], y[q]).\n\nkernelmatrix(κ::Kernel, X::AbstractMatrix; obsdim::Int=2)\nkernelmatrix(κ::Kernel, X::AbstractMatrix, Y::AbstractMatrix; obsdim::Int=2)\n\nEquivalent to kernelmatrix(κ, ColVecs(X)) and kernelmatrix(κ, ColVecs(X), ColVecs(Y)) respectively. Set obsdim=1 to get RowVecs.\n\nSee also: ColVecs, RowVecs\n\n\n\n\n\n","category":"function"},{"location":"api/#KernelFunctions.kernelmatrix!","page":"API","title":"KernelFunctions.kernelmatrix!","text":"kernelmatrix!(K::AbstractMatrix, κ::Kernel, x::AbstractVector)\nkernelmatrix!(K::AbstractMatrix, κ::Kernel, x::AbstractVector, y::AbstractVector)\n\nIn-place version of kernelmatrix where pre-allocated matrix K will be overwritten with the kernel matrix.\n\nkernelmatrix!(K::AbstractMatrix, κ::Kernel, X::AbstractMatrix; obsdim::Integer=2)\nkernelmatrix!(\n    K::AbstractMatrix,\n    κ::Kernel,\n    X::AbstractMatrix,\n    Y::AbstractMatrix;\n    obsdim::Integer=2,\n)\n\nEquivalent to kernelmatrix!(K, κ, ColVecs(X)) and kernelmatrix(K, κ, ColVecs(X), ColVecs(Y)) respectively. Set obsdim=1 to get RowVecs.\n\nSee also: ColVecs, RowVecs\n\n\n\n\n\n","category":"function"},{"location":"api/#KernelFunctions.kernelmatrix_diag","page":"API","title":"KernelFunctions.kernelmatrix_diag","text":"kernelmatrix_diag(κ::Kernel, x::AbstractVector)\n\nCompute the diagonal of kernelmatrix(κ, x) efficiently.\n\nkernelmatrix_diag(κ::Kernel, x::AbstractVector, y::AbstractVector)\n\nCompute the diagonal of kernelmatrix(κ, x, y) efficiently. Requires that x and y are the same length.\n\nkernelmatrix_diag(κ::Kernel, X::AbstractMatrix; obsdim::Int=2)\nkernelmatrix_diag(κ::Kernel, X::AbstractMatrix, Y::AbstractMatrix; obsdim::Int=2)\n\nEquivalent to kernelmatrix_diag(κ, ColVecs(X)) and kernelmatrix_diag(κ, ColVecs(X), ColVecs(Y)) respectively.\n\nSee also: ColVecs, RowVecs\n\n\n\n\n\n","category":"function"},{"location":"api/#KernelFunctions.kernelmatrix_diag!","page":"API","title":"KernelFunctions.kernelmatrix_diag!","text":"kernelmatrix_diag!(K::AbstractVector, κ::Kernel, x::AbstractVector)\nkernelmatrix_diag!(K::AbstractVector, κ::Kernel, x::AbstractVector, y::AbstractVector)\n\nIn place version of kernelmatrix_diag.\n\nkernelmatrix_diag!(K::AbstractVector, κ::Kernel, X::AbstractMatrix; obsdim::Int=2)\nkernelmatrix_diag!(\n    K::AbstractVector,\n    κ::Kernel,\n    X::AbstractMatrix,\n    Y::AbstractMatrix;\n    obsdim::Int=2,\n)\n\nEquivalent to kernelmatrix_diag!(K, κ, ColVecs(X)) and kernelmatrix_diag!(K, κ, ColVecs(X), ColVecs(Y)) respectively. Set obsdim=1 to get RowVecs.\n\nSee also: ColVecs, RowVecs\n\n\n\n\n\n","category":"function"},{"location":"api/#Input-Types","page":"API","title":"Input Types","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"The above API operates on collections of inputs. All collections of inputs in KernelFunctions.jl are represented as AbstractVectors. To understand this choice, please see the design notes on collections of inputs. The length of any such AbstractVector is equal to the number of inputs in the collection. For example, this means that","category":"page"},{"location":"api/","page":"API","title":"API","text":"size(kernelmatrix(k, x)) == (length(x), length(x))","category":"page"},{"location":"api/","page":"API","title":"API","text":"is always true, for some Kernel k, and AbstractVector x.","category":"page"},{"location":"api/#Univariate-Inputs","page":"API","title":"Univariate Inputs","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"If each input to your kernel is Real-valued, then any AbstractVector{<:Real} is a valid representation for a collection of inputs. More generally, it's completely fine to represent a collection of inputs of type T as, for example, a Vector{T}. However, this may not be the most efficient way to represent collection of inputs. See Vector-Valued Inputs for an example.","category":"page"},{"location":"api/#Vector-Valued-Inputs","page":"API","title":"Vector-Valued Inputs","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"We recommend that collections of vector-valued inputs are stored in an AbstractMatrix{<:Real} when possible, and wrapped inside a ColVecs or RowVecs to make their interpretation clear:","category":"page"},{"location":"api/","page":"API","title":"API","text":"ColVecs\nRowVecs","category":"page"},{"location":"api/#KernelFunctions.ColVecs","page":"API","title":"KernelFunctions.ColVecs","text":"ColVecs(X::AbstractMatrix)\n\nA lightweight wrapper for an AbstractMatrix which interprets it as a vector-of-vectors, in which each column of X represents a single vector.\n\nThat is, by writing x = ColVecs(X), you are saying \"x is a vector-of-vectors, each of which has length size(X, 1). The total number of vectors is size(X, 2).\"\n\nPhrased differently, ColVecs(X) says that X should be interpreted as a vector of horizontally-concatenated column-vectors, hence the name ColVecs.\n\njulia> X = randn(2, 5);\n\njulia> x = ColVecs(X);\n\njulia> length(x) == 5\ntrue\n\njulia> X[:, 3] == x[3]\ntrue\n\nColVecs is related to RowVecs via transposition:\n\njulia> X = randn(2, 5);\n\njulia> ColVecs(X) == RowVecs(X')\ntrue\n\n\n\n\n\n","category":"type"},{"location":"api/#KernelFunctions.RowVecs","page":"API","title":"KernelFunctions.RowVecs","text":"RowVecs(X::AbstractMatrix)\n\nA lightweight wrapper for an AbstractMatrix which interprets it as a vector-of-vectors, in which each row of X represents a single vector.\n\nThat is, by writing x = RowVecs(X), you are saying \"x is a vector-of-vectors, each of which has length size(X, 2). The total number of vectors is size(X, 1).\"\n\nPhrased differently, RowVecs(X) says that X should be interpreted as a vector of vertically-concatenated row-vectors, hence the name RowVecs.\n\nInternally, the data continues to be represented as an AbstractMatrix, so using this type does not introduce any kind of performance penalty.\n\njulia> X = randn(5, 2);\n\njulia> x = RowVecs(X);\n\njulia> length(x) == 5\ntrue\n\njulia> X[3, :] == x[3]\ntrue\n\nRowVecs is related to ColVecs via transposition:\n\njulia> X = randn(5, 2);\n\njulia> RowVecs(X) == ColVecs(X')\ntrue\n\n\n\n\n\n","category":"type"},{"location":"api/","page":"API","title":"API","text":"These types are specialised upon to ensure good performance e.g. when computing Euclidean distances between pairs of elements. The benefit of using this representation, rather than using a Vector{Vector{<:Real}}, is that optimised matrix-matrix multiplication functionality can be utilised when computing pairwise distances between inputs, which are needed for kernelmatrix computation.","category":"page"},{"location":"api/#Inputs-for-Multiple-Outputs","page":"API","title":"Inputs for Multiple Outputs","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"KernelFunctions.jl views multi-output GPs as GPs on an extended input domain. For an explanation of this design choice, see the design notes on multi-output GPs.","category":"page"},{"location":"api/","page":"API","title":"API","text":"An input to a multi-output Kernel should be a Tuple{T, Int}, whose first element specifies a location in the domain of the multi-output GP, and whose second element specifies which output the inputs corresponds to. The type of collections of inputs for multi-output GPs is therefore AbstractVector{<:Tuple{T, Int}}.","category":"page"},{"location":"api/","page":"API","title":"API","text":"KernelFunctions.jl provides the following helper function for situations in which all outputs are observed all of the time:","category":"page"},{"location":"api/","page":"API","title":"API","text":"prepare_isotopic_multi_output_data(x::AbstractVector, y::ColVecs)\nprepare_isotopic_multi_output_data(x::AbstractVector, y::RowVecs)","category":"page"},{"location":"api/#KernelFunctions.prepare_isotopic_multi_output_data-Tuple{AbstractVector{T} where T, ColVecs}","page":"API","title":"KernelFunctions.prepare_isotopic_multi_output_data","text":"prepare_isotopic_multi_output_data(x::AbstractVector, y::ColVecs)\n\nUtility functionality to convert a collection of N = length(x) inputs x, and a vector-of-vectors y (efficiently represented by a ColVecs) into a format suitable for use with multi-output kernels.\n\ny[n] is the vector-valued output corresponding to the input x[n]. Consequently, it is necessary that length(x) == length(y).\n\nFor example, if outputs are initially stored in a num_outputs × N matrix:\n\njulia> x = [1.0, 2.0, 3.0];\n\njulia> Y = [1.1 2.1 3.1; 1.2 2.2 3.2]\n2×3 Matrix{Float64}:\n 1.1  2.1  3.1\n 1.2  2.2  3.2\n\njulia> inputs, outputs = prepare_isotopic_multi_output_data(x, ColVecs(Y));\n\njulia> inputs\n6-element KernelFunctions.MOInputIsotopicByFeatures{Float64, Vector{Float64}}:\n (1.0, 1)\n (1.0, 2)\n (2.0, 1)\n (2.0, 2)\n (3.0, 1)\n (3.0, 2)\n\njulia> outputs\n6-element Vector{Float64}:\n 1.1\n 1.2\n 2.1\n 2.2\n 3.1\n 3.2\n\n\n\n\n\n","category":"method"},{"location":"api/#KernelFunctions.prepare_isotopic_multi_output_data-Tuple{AbstractVector{T} where T, RowVecs}","page":"API","title":"KernelFunctions.prepare_isotopic_multi_output_data","text":"prepare_isotopic_multi_output_data(x::AbstractVector, y::RowVecs)\n\nUtility functionality to convert a collection of N = length(x) inputs x and output vectors y (efficiently represented by a RowVecs) into a format suitable for use with multi-output kernels.\n\ny[n] is the vector-valued output corresponding to the input x[n]. Consequently, it is necessary that length(x) == length(y).\n\nFor example, if outputs are initial stored in an N × num_outputs matrix:\n\njulia> x = [1.0, 2.0, 3.0];\n\njulia> Y = [1.1 1.2; 2.1 2.2; 3.1 3.2]\n3×2 Matrix{Float64}:\n 1.1  1.2\n 2.1  2.2\n 3.1  3.2\n\njulia> inputs, outputs = prepare_isotopic_multi_output_data(x, RowVecs(Y));\n\njulia> inputs\n6-element KernelFunctions.MOInputIsotopicByOutputs{Float64, Vector{Float64}}:\n (1.0, 1)\n (2.0, 1)\n (3.0, 1)\n (1.0, 2)\n (2.0, 2)\n (3.0, 2)\n\njulia> outputs\n6-element Vector{Float64}:\n 1.1\n 2.1\n 3.1\n 1.2\n 2.2\n 3.2\n\n\n\n\n\n","category":"method"},{"location":"api/","page":"API","title":"API","text":"The input types that it constructs can also be constructed manually:","category":"page"},{"location":"api/","page":"API","title":"API","text":"MOInput","category":"page"},{"location":"api/#KernelFunctions.MOInput","page":"API","title":"KernelFunctions.MOInput","text":"MOInput(x::AbstractVector, out_dim::Integer)\n\nA data type to accommodate modelling multi-dimensional output data. MOInput(x, out_dim) has length length(x) * out_dim.\n\njulia> x = [1, 2, 3];\n\njulia> MOInput(x, 2)\n6-element KernelFunctions.MOInputIsotopicByOutputs{Int64, Vector{Int64}}:\n (1, 1)\n (2, 1)\n (3, 1)\n (1, 2)\n (2, 2)\n (3, 2)\n\nAs shown above, an MOInput represents a vector of tuples. The first length(x) elements represent the inputs for the first output, the second length(x) elements represent the inputs for the second output, etc. See Inputs for Multiple Outputs in the docs for more info.\n\nMOInput will be deprecated in version 0.11 in favour of MOInputIsotopicByOutputs, and removed in version 0.12.\n\n\n\n\n\n","category":"type"},{"location":"api/","page":"API","title":"API","text":"As with ColVecs and RowVecs for vector-valued input spaces, this type enables specialised implementations of e.g. kernelmatrix for MOInputs in some situations.","category":"page"},{"location":"api/","page":"API","title":"API","text":"To find out more about the background, read this review of kernels for vector-valued functions.","category":"page"},{"location":"api/#Generic-Utilities","page":"API","title":"Generic Utilities","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"KernelFunctions also provides miscellaneous utility functions.","category":"page"},{"location":"api/","page":"API","title":"API","text":"kernelpdmat\nnystrom\nNystromFact","category":"page"},{"location":"api/#KernelFunctions.kernelpdmat","page":"API","title":"KernelFunctions.kernelpdmat","text":"kernelpdmat(k::Kernel, X::AbstractMatrix; obsdim::Int=2)\nkernelpdmat(k::Kernel, X::AbstractVector)\n\nCompute a positive-definite matrix in the form of a PDMat matrix (see PDMats.jl), with the Cholesky decomposition precomputed. The algorithm adds a diagonal \"nugget\" term to the kernel matrix which is increased until positive definiteness is achieved. The algorithm gives up with an error if the nugget becomes larger than 1% of the largest value in the kernel matrix.\n\n\n\n\n\n","category":"function"},{"location":"api/#KernelFunctions.nystrom","page":"API","title":"KernelFunctions.nystrom","text":"nystrom(k::Kernel, X::Matrix, S::Vector; obsdim::Int=defaultobs)\n\nComputes a factorization of Nystrom approximation of the square kernel matrix of data matrix X with respect to kernel k. Returns a NystromFact struct which stores a Nystrom factorization satisfying:\n\nmathbfK approx mathbfC^intercalmathbfWmathbfC\n\n\n\n\n\nnystrom(k::Kernel, X::Matrix, r::Real; obsdim::Int=defaultobs)\n\nComputes a factorization of Nystrom approximation of the square kernel matrix of data matrix X with respect to kernel k using a sample ratio of r. Returns a NystromFact struct which stores a Nystrom factorization satisfying:\n\nmathbfK approx mathbfC^intercalmathbfWmathbfC\n\n\n\n\n\n","category":"function"},{"location":"api/#KernelFunctions.NystromFact","page":"API","title":"KernelFunctions.NystromFact","text":"NystromFact\n\nType for storing a Nystrom factorization. The factorization contains two fields: W and C, two matrices satisfying:\n\nmathbfK approx mathbfC^intercalmathbfWmathbfC\n\n\n\n\n\n","category":"type"},{"location":"design/#Design","page":"Design","title":"Design","text":"","category":"section"},{"location":"design/#why_abstract_vectors","page":"Design","title":"Why AbstractVectors Everywhere?","text":"","category":"section"},{"location":"design/","page":"Design","title":"Design","text":"To understand the advantages of using AbstractVectors everywhere to represent collections of inputs, first consider the following properties that it is desirable for a collection of inputs to satisfy.","category":"page"},{"location":"design/#Unique-Ordering","page":"Design","title":"Unique Ordering","text":"","category":"section"},{"location":"design/","page":"Design","title":"Design","text":"There must be a clearly-defined first, second, etc element of an input collection. If this were not the case, it would not be possible to determine a unique mapping between a collection of inputs and the output of kernelmatrix, as it would not be clear what order the rows and columns of the output should appear in.","category":"page"},{"location":"design/","page":"Design","title":"Design","text":"Moreover, ordering guarantees that if you permute the collection of inputs, the ordering of the rows and columns of the kernelmatrix are correspondingly permuted.","category":"page"},{"location":"design/#Generality","page":"Design","title":"Generality","text":"","category":"section"},{"location":"design/","page":"Design","title":"Design","text":"There must be no restriction on the domain of the input. Collections of Reals, vectors, graphs, finite-dimensional domains, or really anything else that you fancy should be straightforwardly representable. Moreover, whichever input class is chosen should not prevent optimal performance from being obtained.","category":"page"},{"location":"design/#Unambiguously-Defined-Length","page":"Design","title":"Unambiguously-Defined Length","text":"","category":"section"},{"location":"design/","page":"Design","title":"Design","text":"Knowing the length of a collection of inputs is important. For example, a well-defined length guarantees that the size of the output of kernelmatrix, and related functions, are predictable. It also makes it possible to perform internal error-checking that ensures that e.g. there are the same number of inputs in two collections of inputs.","category":"page"},{"location":"design/#AbstractMatrices-Do-Not-Cut-It","page":"Design","title":"AbstractMatrices Do Not Cut It","text":"","category":"section"},{"location":"design/","page":"Design","title":"Design","text":"Notably, while AbstractMatrix objects are often used to represent collections of vector-valued inputs, they do not immediately satisfy these properties as it is unclear whether a matrix of size P x Q represents a collection of P Q-dimensional inputs (each row is an input), or Q P-dimensional inputs (each column is an input).","category":"page"},{"location":"design/","page":"Design","title":"Design","text":"Moreover, they occassionally add some aesthetic inconvenience. For example, a collection of Real-valued inputs, which might be straightforwardly represented as an AbstractVector{<:Real}, must be reshaped into a matrix.","category":"page"},{"location":"design/","page":"Design","title":"Design","text":"There are two commonly used ways to partly resolve these shortcomings:","category":"page"},{"location":"design/#Resolution-1:-Specify-a-Convention","page":"Design","title":"Resolution 1: Specify a Convention","text":"","category":"section"},{"location":"design/","page":"Design","title":"Design","text":"One way that these shortcomings can be partly resolved is by specifying a convention that everyone adheres to regarding the interpretation of rows vs columns. However, opinions about the choice of convention are often surprisingly strongly held, and users regularly have to remind themselves which convention has been chosen. While this resolves the ordering problem, and in principle defines the \"length\" of a collection of inputs, AbstractMatrixs already have a length defined in Julia, which would generally disagree with our internal notion of length. This isn't a show-stopper, but it isn't an especially clean situation.","category":"page"},{"location":"design/","page":"Design","title":"Design","text":"There is also the opportunity for some kinds of silent bugs. For example, if an input matrix happens to be square because the number of input dimensions is the same as the number of inputs, it would be hard to know whether the correct kernelmatrix has been computed. This kind of bug seems unlikely, but it exists regardless.","category":"page"},{"location":"design/","page":"Design","title":"Design","text":"Finally, suppose that your inputs are some type T that is not simply a vector of real numbers, say a graph. In this situation, how should a collection of inputs be represented? A N x 1 or 1 x N matrix is the only obvious candidate, but the additional singular dimension seems somewhat redundant.","category":"page"},{"location":"design/#Resolution-2:-Always-Specify-An-obsdim-Argument","page":"Design","title":"Resolution 2: Always Specify An obsdim Argument","text":"","category":"section"},{"location":"design/","page":"Design","title":"Design","text":"Another way to partly resolve these problems is to not commit to a convention, and instead to propagate some additional information through the codebase that specifies how the input data is to be interpretted. For example, a kernel k that represents the sum of two other kernels might implement kernelmatrix as follows:","category":"page"},{"location":"design/","page":"Design","title":"Design","text":"function kernelmatrix(k::KernelSum, x::AbstractMatrix; obsdim=1)\n    return kernelmatrix(k.kernels[1], x; obsdim=obsdim) +\n        kernelmatrix(k.kernels[2], x; obsdim=obsdim)\nend","category":"page"},{"location":"design/","page":"Design","title":"Design","text":"While this prevents this package from having to pre-specify a convention, it doesn't resolve the length issue, or the issue of representing collections of inputs which aren't immediately represented as vectors. Moreover, it complicates the internals; in contrast, consider what this function looks like with an AbstractVector:","category":"page"},{"location":"design/","page":"Design","title":"Design","text":"function kernelmatrix(k::KernelSum, x::AbstractVector)\n    return kernelmatrix(k.kernels[1], x) + kernelmatrix(k.kernels[2], x)\nend","category":"page"},{"location":"design/","page":"Design","title":"Design","text":"This code is clearer (less visual noise), and has removed a possible bug – if the implementer of kernelmatrix forgets to pass the obsdim kwarg into each subsequent kernelmatrix call, it's possible to get the wrong answer.","category":"page"},{"location":"design/","page":"Design","title":"Design","text":"This being said, we do support matrix-valued inputs – see Why We Have Support for Both.","category":"page"},{"location":"design/#AbstractVectors","page":"Design","title":"AbstractVectors","text":"","category":"section"},{"location":"design/","page":"Design","title":"Design","text":"Requiring all collections of inputs to be AbstractVectors resolves all of these problems, and ensures that the data is self-describing to the extent that KernelFunctions.jl requires.","category":"page"},{"location":"design/","page":"Design","title":"Design","text":"Firstly, the question of how to interpret the columns and rows of a matrix of inputs is resolved. Users must wrap matrices which represent collections of inputs in either a ColVecs or RowVecs, both of which have clearly defined semantics which are hard to confuse.","category":"page"},{"location":"design/","page":"Design","title":"Design","text":"By design, there is also no discrepancy between the number of inputs in the collection, and the length function – the length of a ColVecs, RowVecs, or Vector{<:Real} is equal to the number of inputs.","category":"page"},{"location":"design/","page":"Design","title":"Design","text":"There is no loss of performance.","category":"page"},{"location":"design/","page":"Design","title":"Design","text":"A collection of N Real-valued inputs can be represented by an AbstractVector{<:Real} of length N, rather than needing to use an AbstractMatrix{<:Real} of size either N x 1 or 1 x N. The same can be said for any other input type T, and new subtypes of AbstractVector can be added if particularly efficient ways exist to store collections of inputs of type T. A good example of this in practice is using Tuple{S, Int}, for some input type S, as the Inputs for Multiple Outputs.","category":"page"},{"location":"design/","page":"Design","title":"Design","text":"This approach can also lead to clearer user code. A user need only wrap their inputs in a ColVecs or RowVecs once in their code, and this specification is automatically re-used everywhere in their code. In this sense, it is straightforward to write code in such a way that there is one unique source of \"truth\" about the way in which a particular data set should be interpreted. Conversely, the obsdim resolution requires that the obsdim keyword argument is passed around with the data every single time that you use it.","category":"page"},{"location":"design/","page":"Design","title":"Design","text":"The benefits of the AbstractVector approach are likely most strongly felt when writing a substantial amount of code on top of KernelFunctions.jl – in the same way that using AbstractVectors inside KernelFunctions.jl removes the need for large amounts of keyword argument propagation, the same will be true of other code.","category":"page"},{"location":"design/#Why-We-Have-Support-for-Both","page":"Design","title":"Why We Have Support for Both","text":"","category":"section"},{"location":"design/","page":"Design","title":"Design","text":"In short: many people like matrices, and are familiar with obsdim-style keyword arguments.","category":"page"},{"location":"design/","page":"Design","title":"Design","text":"All internals are implemented using AbstractVectors though, and the obsdim interface is just a thin layer of utility functionality which sits on top of this.","category":"page"},{"location":"design/#inputs_for_multiple_outputs","page":"Design","title":"Kernels for Multiple-Outputs","text":"","category":"section"},{"location":"design/","page":"Design","title":"Design","text":"There are two equally-valid perspectives on multi-output kernels: they can either be treated as matrix-valued kernels, or standard kernels on an extended input domain. Each of these perspectives are convenient in different circumstances, but the latter greatly simplifies the incorporation of multi-output kernels in KernelFunctions.","category":"page"},{"location":"design/","page":"Design","title":"Design","text":"More concretely, let k_mat be a matrix-valued kernel, mapping pairs of inputs of type T to matrices of size P x P to describe the covariance between P outputs. Given inputs x and y of type T, and integers p and q, we can always find an equivalent standard kernel k mapping from pairs of inputs of type Tuple{T, Int} to the Reals as follows:","category":"page"},{"location":"design/","page":"Design","title":"Design","text":"k((x, p), (y, q)) = k_mat(x, y)[p, q]","category":"page"},{"location":"design/","page":"Design","title":"Design","text":"This ability to treat multi-output kernels as single-output kernels is very helpful, as it means that there is no need to introduce additional concepts into the API of KernelFunctions.jl, just additional kernels! This in turn simplifies downstream code as they don't need to \"know\" about the existence of multi-output kernels in addition to standard kernels. For example, GP libraries built on top of KernelFunctions.jl just need to know about Kernels, and they get multi-output kernels, and hence multi-output GPs, for free.","category":"page"},{"location":"design/","page":"Design","title":"Design","text":"Where there is the need to specialise implementations for multi-output kernels, this is done in an encapsulated manner – parts of KernelFunctions that have nothing to do with multi-output kernels know nothing about the existence of multi-output kernels.","category":"page"},{"location":"metrics/#Metrics","page":"Metrics","title":"Metrics","text":"","category":"section"},{"location":"metrics/","page":"Metrics","title":"Metrics","text":"SimpleKernel implementations rely on Distances.jl for efficiently computing the pairwise matrix. This requires a distance measure or metric, such as the commonly used SqEuclidean and Euclidean.","category":"page"},{"location":"metrics/","page":"Metrics","title":"Metrics","text":"The metric used by a given kernel type is specified as","category":"page"},{"location":"metrics/","page":"Metrics","title":"Metrics","text":"KernelFunctions.metric(::CustomKernel) = SqEuclidean()","category":"page"},{"location":"metrics/","page":"Metrics","title":"Metrics","text":"However, there are kernels that can be implemented efficiently using \"metrics\" that do not respect all the definitions expected by Distances.jl. For this reason, KernelFunctions.jl provides additional \"metrics\" such as DotProduct (langle x y rangle) and Delta (delta(xy)).","category":"page"},{"location":"metrics/#Adding-a-new-metric","page":"Metrics","title":"Adding a new metric","text":"","category":"section"},{"location":"metrics/","page":"Metrics","title":"Metrics","text":"If you want to create a new \"metric\" just implement the following:","category":"page"},{"location":"metrics/","page":"Metrics","title":"Metrics","text":"struct Delta <: Distances.PreMetric\nend\n\n@inline function Distances._evaluate(::Delta,a::AbstractVector{T},b::AbstractVector{T}) where {T}\n    @boundscheck if length(a) != length(b)\n        throw(DimensionMismatch(\"first array has length $(length(a)) which does not match the length of the second, $(length(b)).\"))\n    end\n    return a==b\nend\n\n@inline (dist::Delta)(a::AbstractArray,b::AbstractArray) = Distances._evaluate(dist,a,b)\n@inline (dist::Delta)(a::Number,b::Number) = a==b","category":"page"},{"location":"transform/#input_transforms","page":"Input Transforms","title":"Input Transforms","text":"","category":"section"},{"location":"transform/#Overview","page":"Input Transforms","title":"Overview","text":"","category":"section"},{"location":"transform/","page":"Input Transforms","title":"Input Transforms","text":"Transforms are designed to change input data before passing it on to a kernel object.","category":"page"},{"location":"transform/","page":"Input Transforms","title":"Input Transforms","text":"It can be as standard as IdentityTransform returning the same input, or multiplying the data by a scalar with ScaleTransform or by a vector with ARDTransform. There is a more general FunctionTransform that uses a function and applies it to each input.","category":"page"},{"location":"transform/","page":"Input Transforms","title":"Input Transforms","text":"You can also create a pipeline of Transforms via ChainTransform, e.g.,","category":"page"},{"location":"transform/","page":"Input Transforms","title":"Input Transforms","text":"LowRankTransform(rand(10, 5)) ∘ ScaleTransform(2.0)","category":"page"},{"location":"transform/","page":"Input Transforms","title":"Input Transforms","text":"A transformation t can be applied to a single input x with t(x) and to multiple inputs xs with map(t, xs).","category":"page"},{"location":"transform/","page":"Input Transforms","title":"Input Transforms","text":"Kernels can be coupled with input transformations with ∘ or its alias compose. It falls back to creating a TransformedKernel but allows more optimized implementations for specific kernels and transformations.","category":"page"},{"location":"transform/#List-of-Input-Transforms","page":"Input Transforms","title":"List of Input Transforms","text":"","category":"section"},{"location":"transform/","page":"Input Transforms","title":"Input Transforms","text":"Transform\nIdentityTransform\nScaleTransform\nARDTransform\nARDTransform(::Real, ::Integer)\nLinearTransform\nFunctionTransform\nSelectTransform\nChainTransform\nPeriodicTransform","category":"page"},{"location":"transform/#KernelFunctions.Transform","page":"Input Transforms","title":"KernelFunctions.Transform","text":"Transform\n\nAbstract type defining a transformation of the input.\n\n\n\n\n\n","category":"type"},{"location":"transform/#KernelFunctions.IdentityTransform","page":"Input Transforms","title":"KernelFunctions.IdentityTransform","text":"IdentityTransform()\n\nTransformation that returns exactly the input.\n\n\n\n\n\n","category":"type"},{"location":"transform/#KernelFunctions.ScaleTransform","page":"Input Transforms","title":"KernelFunctions.ScaleTransform","text":"ScaleTransform(l::Real)\n\nTransformation that multiplies the input elementwise with l.\n\nExamples\n\njulia> l = rand(); t = ScaleTransform(l); X = rand(100, 10);\n\njulia> map(t, ColVecs(X)) == ColVecs(l .* X)\ntrue\n\n\n\n\n\n","category":"type"},{"location":"transform/#KernelFunctions.ARDTransform","page":"Input Transforms","title":"KernelFunctions.ARDTransform","text":"ARDTransform(v::AbstractVector)\n\nTransformation that multiplies the input elementwise by v.\n\nExamples\n\njulia> v = rand(10); t = ARDTransform(v); X = rand(10, 100);\n\njulia> map(t, ColVecs(X)) == ColVecs(v .* X)\ntrue\n\n\n\n\n\n","category":"type"},{"location":"transform/#KernelFunctions.ARDTransform-Tuple{Real, Integer}","page":"Input Transforms","title":"KernelFunctions.ARDTransform","text":"ARDTransform(s::Real, dims::Integer)\n\nCreate an ARDTransform with vector fill(s, dims).\n\n\n\n\n\n","category":"method"},{"location":"transform/#KernelFunctions.LinearTransform","page":"Input Transforms","title":"KernelFunctions.LinearTransform","text":"LinearTransform(A::AbstractMatrix)\n\nLinear transformation of the input realised by the matrix A.\n\nThe second dimension of A must match the number of features of the target.\n\nExamples\n\njulia> A = rand(10, 5); t = LinearTransform(A); X = rand(5, 100);\n\njulia> map(t, ColVecs(X)) == ColVecs(A * X)\ntrue\n\n\n\n\n\n","category":"type"},{"location":"transform/#KernelFunctions.FunctionTransform","page":"Input Transforms","title":"KernelFunctions.FunctionTransform","text":"FunctionTransform(f)\n\nTransformation that applies function f to the input.\n\nMake sure that f can act on an input. For instance, if the inputs are vectors, use f(x) = sin.(x) instead of f = sin.\n\nExamples\n\njulia> f(x) = sum(x); t = FunctionTransform(f); X = randn(100, 10);\n\njulia> map(t, ColVecs(X)) == ColVecs(sum(X; dims=1))\ntrue\n\n\n\n\n\n","category":"type"},{"location":"transform/#KernelFunctions.SelectTransform","page":"Input Transforms","title":"KernelFunctions.SelectTransform","text":"SelectTransform(dims)\n\nTransformation that selects the dimensions dims of the input.\n\nExamples\n\njulia> dims = [1, 3, 5, 6, 7]; t = SelectTransform(dims); X = rand(100, 10);\n\njulia> map(t, ColVecs(X)) == ColVecs(X[dims, :])\ntrue\n\n\n\n\n\n","category":"type"},{"location":"transform/#KernelFunctions.ChainTransform","page":"Input Transforms","title":"KernelFunctions.ChainTransform","text":"ChainTransform(ts::AbstractVector{<:Transform})\n\nTransformation that applies a chain of transformations ts to the input.\n\nThe transformation first(ts) is applied first.\n\nExamples\n\njulia> l = rand(); A = rand(3, 4); t1 = ScaleTransform(l); t2 = LinearTransform(A);\n\njulia> X = rand(4, 10);\n\njulia> map(ChainTransform([t1, t2]), ColVecs(X)) == ColVecs(A * (l .* X))\ntrue\n\njulia> map(t2 ∘ t1, ColVecs(X)) == ColVecs(A * (l .* X))\ntrue\n\n\n\n\n\n","category":"type"},{"location":"transform/#KernelFunctions.PeriodicTransform","page":"Input Transforms","title":"KernelFunctions.PeriodicTransform","text":"PeriodicTransform(f)\n\nTransformation that maps the input elementwise onto the unit circle with frequency f.\n\nSamples from a GP with a kernel with this transformation applied to the inputs will produce samples with frequency f.\n\nExamples\n\njulia> f = rand(); t = PeriodicTransform(f); x = rand();\n\njulia> t(x) == [sinpi(2 * f * x), cospi(2 * f * x)]\ntrue\n\n\n\n\n\n","category":"type"},{"location":"transform/#Convenience-functions","page":"Input Transforms","title":"Convenience functions","text":"","category":"section"},{"location":"transform/","page":"Input Transforms","title":"Input Transforms","text":"with_lengthscale","category":"page"},{"location":"transform/#KernelFunctions.with_lengthscale","page":"Input Transforms","title":"KernelFunctions.with_lengthscale","text":"with_lengthscale(kernel::Kernel, lengthscale::Real)\n\nConstruct a transformed kernel with lengthscale.\n\nExamples\n\njulia> kernel = with_lengthscale(SqExponentialKernel(), 2.5);\n\njulia> x = rand(2);\n\njulia> y = rand(2);\n\njulia> kernel(x, y) ≈ (SqExponentialKernel() ∘ ScaleTransform(0.4))(x, y)\ntrue\n\n\n\n\n\nwith_lengthscale(kernel::Kernel, lengthscales::AbstractVector{<:Real})\n\nConstruct a transformed \"ARD\" kernel with different lengthscales for each dimension.\n\nExamples\n\njulia> kernel = with_lengthscale(SqExponentialKernel(), [0.5, 2.5]);\n\njulia> x = rand(2);\n\njulia> y = rand(2);\n\njulia> kernel(x, y) ≈ (SqExponentialKernel() ∘ ARDTransform([2, 0.4]))(x, y)\ntrue\n\n\n\n\n\n","category":"function"},{"location":"userguide/#User-guide","page":"User guide","title":"User guide","text":"","category":"section"},{"location":"userguide/#Kernel-Creation","page":"User guide","title":"Kernel Creation","text":"","category":"section"},{"location":"userguide/","page":"User guide","title":"User guide","text":"To create a kernel object, choose one of the pre-implemented kernels, see Kernel Functions, or create your own, see Creating your own kernel. For example, a squared exponential kernel is created by","category":"page"},{"location":"userguide/","page":"User guide","title":"User guide","text":"  k = SqExponentialKernel()","category":"page"},{"location":"userguide/","page":"User guide","title":"User guide","text":"tip: How do I set the lengthscale?\nInstead of having lengthscale(s) for each kernel we use Transform objects which act on the inputs before passing them to the kernel. Note that the transforms such as ScaleTransform and ARDTransform multiply the input by a scale factor, which corresponds to the inverse of the lengthscale. For example, a lengthscale of 0.5 is equivalent to premultiplying the input by 2.0, and you can create the corresponding kernel in either of the following equivalent ways:  k = SqExponentialKernel() ∘ ScaleTransform(2.0)\n  k = compose(SqExponentialKernel(), ScaleTransform(2.0))Alternatively, you can use the convenience function with_lengthscale:k = with_lengthscale(SqExponentialKernel(), 0.5)with_lengthscale also works with vector-valued lengthscales for ARD. Check the Input Transforms page for more details.","category":"page"},{"location":"userguide/","page":"User guide","title":"User guide","text":"tip: How do I set the kernel variance?\nTo premultiply the kernel by a variance, you can use * with a scalar number:  k = 3.0 * SqExponentialKernel()","category":"page"},{"location":"userguide/","page":"User guide","title":"User guide","text":"tip: How do I use a Mahalanobis kernel?\nThe MahalanobisKernel(; P=P), defined byk(x x P) = expbig(- (x - x)^top P (x - x)big)for a positive definite matrix P = Q^top Q, was removed in 0.9. Instead you can use a squared exponential kernel together with a LinearTransform of the inputs:k = SqExponentialKernel() ∘ LinearTransform(sqrt(2) .* Q)Analogously, you can combine other kernels such as the PiecewisePolynomialKernel with a LinearTransform of the inputs to obtain a kernel that is a function of the Mahalanobis distance between inputs.","category":"page"},{"location":"userguide/#Using-a-Kernel-Function","page":"User guide","title":"Using a Kernel Function","text":"","category":"section"},{"location":"userguide/","page":"User guide","title":"User guide","text":"To evaluate the kernel function on two vectors you simply call the kernel object:","category":"page"},{"location":"userguide/","page":"User guide","title":"User guide","text":"k = SqExponentialKernel()\nx1 = rand(3)\nx2 = rand(3)\nk(x1, x2)","category":"page"},{"location":"userguide/#Creating-a-Kernel-Matrix","page":"User guide","title":"Creating a Kernel Matrix","text":"","category":"section"},{"location":"userguide/","page":"User guide","title":"User guide","text":"Kernel matrices can be created via the kernelmatrix function or kernelmatrix_diag for only the diagonal. For example, for a collection of 10 Real-valued inputs:","category":"page"},{"location":"userguide/","page":"User guide","title":"User guide","text":"k = SqExponentialKernel()\nx = rand(10)\nkernelmatrix(k, x) # 10x10 matrix","category":"page"},{"location":"userguide/","page":"User guide","title":"User guide","text":"If your inputs are multi-dimensional, it is common to represent them as a matrix. For example","category":"page"},{"location":"userguide/","page":"User guide","title":"User guide","text":"X = rand(10, 5)","category":"page"},{"location":"userguide/","page":"User guide","title":"User guide","text":"However, it is ambiguous whether this represents a collection of 10 5-dimensional row-vectors, or 5 10-dimensional column-vectors. Therefore, we require users to provide some more information.","category":"page"},{"location":"userguide/","page":"User guide","title":"User guide","text":"You can write RowVecs(X) to declare that X contains 10 5-dimensional row-vectors, or ColVecs(X) to declare that X contains 5 10-dimensional column-vectors, then","category":"page"},{"location":"userguide/","page":"User guide","title":"User guide","text":"kernelmatrix(k, RowVecs(X))  # returns a 10×10 matrix -- each row of X treated as input\nkernelmatrix(k, ColVecs(X))  # returns a 5×5 matrix -- each column of X treated as input","category":"page"},{"location":"userguide/","page":"User guide","title":"User guide","text":"This is the mechanism used throughout KernelFunctions.jl to handle multi-dimensional inputs.","category":"page"},{"location":"userguide/","page":"User guide","title":"User guide","text":"You can utilise the obsdim keyword argument if you prefer:","category":"page"},{"location":"userguide/","page":"User guide","title":"User guide","text":"kernelmatrix(k, X; obsdim=1) # same as RowVecs(X)\nkernelmatrix(k, X; obsdim=2) # same as ColVecs(X)","category":"page"},{"location":"userguide/","page":"User guide","title":"User guide","text":"This is similar to the convention used in Distances.jl.","category":"page"},{"location":"userguide/#So-what-type-should-I-use-to-represent-a-collection-of-inputs?","page":"User guide","title":"So what type should I use to represent a collection of inputs?","text":"","category":"section"},{"location":"userguide/","page":"User guide","title":"User guide","text":"The central assumption made by KernelFunctions.jl is that all collections of N inputs are represented by AbstractVectors of length N. Abstraction is then used to ensure that efficiency is retained, ColVecs and RowVecs being the most obvious examples of this.","category":"page"},{"location":"userguide/","page":"User guide","title":"User guide","text":"Concretely:","category":"page"},{"location":"userguide/","page":"User guide","title":"User guide","text":"For Real-valued inputs (scalars), a Vector{<:Real} is fine.\nFor vector-valued inputs, consider a ColVecs or RowVecs.\nFor a new input type, simply represent collections of inputs of this type as an AbstractVector.","category":"page"},{"location":"userguide/","page":"User guide","title":"User guide","text":"See Input Types and Design for a more thorough discussion of the considerations made when this design was adopted.","category":"page"},{"location":"userguide/","page":"User guide","title":"User guide","text":"The obsdim kwarg mentioned above is a special case for vector-valued inputs stored in a matrix. It is implemented as a lightweight wrapper that constructs either a RowVecs or ColVecs from your inputs, and passes this on.","category":"page"},{"location":"userguide/#Output-Types","page":"User guide","title":"Output Types","text":"","category":"section"},{"location":"userguide/","page":"User guide","title":"User guide","text":"In addition to plain Matrix-like output, KernelFunctions.jl supports specific output types:","category":"page"},{"location":"userguide/","page":"User guide","title":"User guide","text":"For a positive-definite matrix object of type PDMat from PDMats.jl, you can call the following:","category":"page"},{"location":"userguide/","page":"User guide","title":"User guide","text":"using PDMats\nk = SqExponentialKernel()\nK = kernelpdmat(k, RowVecs(X)) # PDMat\nK = kernelpdmat(k, X; obsdim=1) # PDMat","category":"page"},{"location":"userguide/","page":"User guide","title":"User guide","text":"It will create a matrix and in case of bad conditioning will add some diagonal noise until the matrix is considered positive-definite; it will then return a PDMat object. For this method to work in your code you need to include using PDMats first.","category":"page"},{"location":"userguide/","page":"User guide","title":"User guide","text":"For a Kronecker matrix, we rely on Kronecker.jl. Here are two examples:","category":"page"},{"location":"userguide/","page":"User guide","title":"User guide","text":"using Kronecker\nx = range(0, 1; length=10)\ny = range(0, 1; length=50)\nK = kernelkronmat(k, [x, y]) # Kronecker matrix\nK = kernelkronmat(k, x, 5) # Kronecker matrix","category":"page"},{"location":"userguide/","page":"User guide","title":"User guide","text":"Make sure that k is a kernel compatible with such constructions (with iskroncompatible(k)). Both methods will return a Kronecker matrix. For those methods to work in your code you need to include using Kronecker first.","category":"page"},{"location":"userguide/","page":"User guide","title":"User guide","text":"For a Nystrom approximation: kernelmatrix(nystrom(k, X, ρ, obsdim=1)) where ρ is the fraction of data samples used in the approximation.","category":"page"},{"location":"userguide/#Composite-Kernels","page":"User guide","title":"Composite Kernels","text":"","category":"section"},{"location":"userguide/","page":"User guide","title":"User guide","text":"Sums and products of kernels are also valid kernels. They can be created via KernelSum and KernelProduct or using simple operators + and *. For example:","category":"page"},{"location":"userguide/","page":"User guide","title":"User guide","text":"k1 = SqExponentialKernel()\nk2 = Matern32Kernel()\nk = 0.5 * k1 + 0.2 * k2 # KernelSum\nk = k1 * k2 # KernelProduct","category":"page"},{"location":"userguide/#Kernel-Parameters","page":"User guide","title":"Kernel Parameters","text":"","category":"section"},{"location":"userguide/","page":"User guide","title":"User guide","text":"What if you want to differentiate through the kernel parameters? This is easy even in a highly nested structure such as:","category":"page"},{"location":"userguide/","page":"User guide","title":"User guide","text":"k = (\n    0.5 * SqExponentialKernel() * Matern12Kernel() +\n    0.2 * (LinearKernel() ∘ ScaleTransform(2.0) + PolynomialKernel())\n) ∘ ARDTransform([0.1, 0.5])","category":"page"},{"location":"userguide/","page":"User guide","title":"User guide","text":"One can access the named tuple of trainable parameters via Functors.functor from Functors.jl. This means that in practice you can implicitly optimize the kernel parameters by calling:","category":"page"},{"location":"userguide/","page":"User guide","title":"User guide","text":"using Flux\nkernelparams = Flux.params(k)\nFlux.gradient(kernelparams) do\n    # ... some loss function on the kernel ....\nend","category":"page"},{"location":"examples/deep-kernel-learning/","page":"Deep Kernel Learning","title":"Deep Kernel Learning","text":"EditURL = \"https://github.com/JuliaGaussianProcesses/KernelFunctions.jl/blob/master/examples/deep-kernel-learning/script.jl\"","category":"page"},{"location":"examples/deep-kernel-learning/#Deep-Kernel-Learning","page":"Deep Kernel Learning","title":"Deep Kernel Learning","text":"","category":"section"},{"location":"examples/deep-kernel-learning/","page":"Deep Kernel Learning","title":"Deep Kernel Learning","text":"(Image: )","category":"page"},{"location":"examples/deep-kernel-learning/","page":"Deep Kernel Learning","title":"Deep Kernel Learning","text":"You are seeing the HTML output generated by Documenter.jl and Literate.jl from the Julia source file. The corresponding notebook can be viewed in nbviewer.","category":"page"},{"location":"examples/deep-kernel-learning/","page":"Deep Kernel Learning","title":"Deep Kernel Learning","text":"warning: Warning\nThis example is under construction","category":"page"},{"location":"examples/deep-kernel-learning/","page":"Deep Kernel Learning","title":"Deep Kernel Learning","text":"Setup","category":"page"},{"location":"examples/deep-kernel-learning/","page":"Deep Kernel Learning","title":"Deep Kernel Learning","text":"using KernelFunctions\nusing MLDataUtils\nusing Zygote\nusing Flux\nusing Distributions, LinearAlgebra\nusing Plots\n\nFlux.@functor SqExponentialKernel\nFlux.@functor KernelSum\nFlux.@functor Matern32Kernel\nFlux.@functor FunctionTransform","category":"page"},{"location":"examples/deep-kernel-learning/","page":"Deep Kernel Learning","title":"Deep Kernel Learning","text":"set up a kernel with a neural network feature extractor:","category":"page"},{"location":"examples/deep-kernel-learning/","page":"Deep Kernel Learning","title":"Deep Kernel Learning","text":"neuralnet = Chain(Dense(1, 3), Dense(3, 2))\nk = SqExponentialKernel() ∘ FunctionTransform(neuralnet)","category":"page"},{"location":"examples/deep-kernel-learning/","page":"Deep Kernel Learning","title":"Deep Kernel Learning","text":"Squared Exponential Kernel (metric = Distances.Euclidean(0.0))\n\t- Function Transform: Chain(Dense(1, 3), Dense(3, 2))","category":"page"},{"location":"examples/deep-kernel-learning/","page":"Deep Kernel Learning","title":"Deep Kernel Learning","text":"Generate date","category":"page"},{"location":"examples/deep-kernel-learning/","page":"Deep Kernel Learning","title":"Deep Kernel Learning","text":"xmin = -3;\nxmax = 3;\nx = range(xmin, xmax; length=100)\nx_test = rand(Uniform(xmin, xmax), 200)\nx, y = noisy_function(sinc, x; noise=0.1)\nX = RowVecs(reshape(x, :, 1))\nX_test = RowVecs(reshape(x_test, :, 1))\nλ = [0.1]","category":"page"},{"location":"examples/deep-kernel-learning/","page":"Deep Kernel Learning","title":"Deep Kernel Learning","text":"f(x, k, λ) = kernelmatrix(k, x, X) / (kernelmatrix(k, X) + exp(λ[1]) * I) * y\nf(X, k, 1.0)","category":"page"},{"location":"examples/deep-kernel-learning/","page":"Deep Kernel Learning","title":"Deep Kernel Learning","text":"100-element Vector{Float64}:\n -0.031811115361666566\n -0.0235933092067251\n -0.01521473310088405\n -0.006686350534060321\n  0.001980374290881516\n  0.010773495780318986\n  0.019680607992012156\n  0.028688867358724914\n  0.037785017047961125\n  0.04695541290573141\n  0.05618605092555489\n  0.06546259617737028\n  0.07477041312463446\n  0.0840945972517067\n  0.0934200079176139\n  0.10273130234661257\n  0.11201297066048574\n  0.12124937185238918\n  0.13042477059729174\n  0.1395233747895803\n  0.14852937369439664\n  0.15742697659560356\n  0.16620045182009183\n  0.1748341660153934\n  0.18331262355524555\n  0.19162050594599975\n  0.1997427111054164\n  0.20766439238462142\n  0.21537099720370437\n  0.2228483051716838\n  0.23008246556233283\n  0.23706003401865716\n  0.2437680083606327\n  0.2501938633731572\n  0.2563255844540355\n  0.26215170000517646\n  0.2676613124540438\n  0.2728441277967506\n  0.27769048355899234\n  0.2821913750762671\n  0.2863384800005073\n  0.2901241809463487\n  0.2935415861966826\n  0.2965845483939866\n  0.29924768115103006\n  0.3015263735219754\n  0.30341680228257834\n  0.3049159419760788\n  0.30602157268948077\n  0.3067322855331425\n  0.3070474858049797\n  0.30696739382901606\n  0.30649304346650147\n  0.3056262783063209\n  0.30436974554985136\n  0.30272688761385264\n  0.30070193148322\n  0.29829987585359935\n  0.2955264761118004\n  0.2923882272097178\n  0.28889234449494144\n  0.2850467425684761\n  0.28086001224688906\n  0.27634139571277433\n  0.2715007599436157\n  0.26634856851494537\n  0.26089585187906783\n  0.25515417622557734\n  0.24913561103437712\n  0.24285269543592566\n  0.2363184034969524\n  0.2295461085529069\n  0.22254954671091237\n  0.21534277964897688\n  0.20794015683867698\n  0.20035627731948003\n  0.1926059511532524\n  0.184704160687424\n  0.17666602175461807\n  0.16850674493544274\n  0.16024159700948612\n  0.151885862717447\n  0.14345480695472904\n  0.13496363751378526\n  0.12642746848901604\n  0.11786128445412321\n  0.1092799055175296\n  0.10069795335683004\n  0.092129818328228\n  0.08358962774159506\n  0.07509121538620281\n  0.06664809238630395\n  0.058273419459653704\n  0.0499799806457778\n  0.04178015856433978\n  0.03368591125736986\n  0.025708750662419854\n  0.017859722756947985\n  0.010149389407427082\n  0.0025878119498421353","category":"page"},{"location":"examples/deep-kernel-learning/","page":"Deep Kernel Learning","title":"Deep Kernel Learning","text":"loss(k, λ) = (ŷ -> sum(y - ŷ) / length(y) + exp(λ[1]) * norm(ŷ))(f(X, k, λ))\nloss(k, λ)","category":"page"},{"location":"examples/deep-kernel-learning/","page":"Deep Kernel Learning","title":"Deep Kernel Learning","text":"2.496911947037652","category":"page"},{"location":"examples/deep-kernel-learning/","page":"Deep Kernel Learning","title":"Deep Kernel Learning","text":"ps = Flux.params(k)","category":"page"},{"location":"examples/deep-kernel-learning/","page":"Deep Kernel Learning","title":"Deep Kernel Learning","text":"Params([Float32[-0.9224117; -0.060308915; 0.39964408], Float32[0.0, 0.0, 0.0], Float32[0.22606009 -1.0537014 0.1883021; -0.618008 -1.086676 -0.77337784], Float32[0.0, 0.0]])","category":"page"},{"location":"examples/deep-kernel-learning/","page":"Deep Kernel Learning","title":"Deep Kernel Learning","text":"push!(ps,λ)","category":"page"},{"location":"examples/deep-kernel-learning/","page":"Deep Kernel Learning","title":"Deep Kernel Learning","text":"opt = Flux.Momentum(1.0)","category":"page"},{"location":"examples/deep-kernel-learning/","page":"Deep Kernel Learning","title":"Deep Kernel Learning","text":"plots = []\nfor i in 1:10\n    grads = Zygote.gradient(() -> loss(k, λ), ps)\n    Flux.Optimise.update!(opt, ps, grads)\n    p = Plots.scatter(x, y; lab=\"data\", title=\"Loss = $(loss(k,λ))\")\n    Plots.plot!(x, f(X, k, λ); lab=\"Prediction\", lw=3.0)\n    push!(plots, p)\nend","category":"page"},{"location":"examples/deep-kernel-learning/","page":"Deep Kernel Learning","title":"Deep Kernel Learning","text":"l = @layout grid(10, 1)\nplot(plots...; layout=l, size=(300, 1500))","category":"page"},{"location":"examples/deep-kernel-learning/","page":"Deep Kernel Learning","title":"Deep Kernel Learning","text":"(Image: )","category":"page"},{"location":"examples/deep-kernel-learning/","page":"Deep Kernel Learning","title":"Deep Kernel Learning","text":"","category":"page"},{"location":"examples/deep-kernel-learning/","page":"Deep Kernel Learning","title":"Deep Kernel Learning","text":"This page was generated using Literate.jl.","category":"page"},{"location":"#KernelFunctions.jl","page":"Home","title":"KernelFunctions.jl","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"KernelFunctions.jl is a general purpose kernel package. It aims at providing a flexible framework for creating kernels and manipulating them. The main goals of this package are:","category":"page"},{"location":"","page":"Home","title":"Home","text":"Flexibility: operations between kernels should be fluid and easy without breaking.\nPlug-and-play: including the kernels before/after other steps should be straightforward.\nAutomatic Differentation compatibility: all kernel functions which ought to be differentiable using AD packages like ForwardDiff.jl or Zygote.jl should be.","category":"page"},{"location":"","page":"Home","title":"Home","text":"This package builds on top of lots of excellent existing work in packages such as MLKernels.jl, Stheno.jl, GaussianProcesses.jl, and AugmentedGaussianProcesses.jl.","category":"page"},{"location":"","page":"Home","title":"Home","text":"See the User guide for a brief introduction.","category":"page"}]
}
