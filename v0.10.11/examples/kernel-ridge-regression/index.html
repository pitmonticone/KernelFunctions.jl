<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Kernel Ridge Regression · KernelFunctions.jl</title><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.039/juliamono-regular.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.11/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../../">KernelFunctions.jl</a></span></div><form class="docs-search" action="../../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li><li><a class="tocitem" href="../../userguide/">User guide</a></li><li><a class="tocitem" href="../../kernels/">Kernel Functions</a></li><li><a class="tocitem" href="../../transform/">Input Transforms</a></li><li><a class="tocitem" href="../../metrics/">Metrics</a></li><li><a class="tocitem" href="../../create_kernel/">Custom Kernels</a></li><li><a class="tocitem" href="../../api/">API</a></li><li><a class="tocitem" href="../../design/">Design</a></li><li><span class="tocitem">Examples</span><ul><li><a class="tocitem" href="../deep-kernel-learning/">Deep Kernel Learning</a></li><li><a class="tocitem" href="../gaussian-process-priors/">Gaussian process prior samples</a></li><li class="is-active"><a class="tocitem" href>Kernel Ridge Regression</a><ul class="internal"><li><a class="tocitem" href="#Toy-data"><span>Toy data</span></a></li><li><a class="tocitem" href="#Linear-regression"><span>Linear regression</span></a></li><li><a class="tocitem" href="#Featurization"><span>Featurization</span></a></li><li><a class="tocitem" href="#Ridge-regression"><span>Ridge regression</span></a></li><li><a class="tocitem" href="#Kernel-ridge-regression"><span>Kernel ridge regression</span></a></li></ul></li><li><a class="tocitem" href="../support-vector-machine/">Support Vector Machine</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Examples</a></li><li class="is-active"><a href>Kernel Ridge Regression</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Kernel Ridge Regression</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/JuliaGaussianProcesses/KernelFunctions.jl/blob/master/examples/kernel-ridge-regression/script.jl" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Kernel-Ridge-Regression"><a class="docs-heading-anchor" href="#Kernel-Ridge-Regression">Kernel Ridge Regression</a><a id="Kernel-Ridge-Regression-1"></a><a class="docs-heading-anchor-permalink" href="#Kernel-Ridge-Regression" title="Permalink"></a></h1><p><a href="https://nbviewer.jupyter.org/github/JuliaGaussianProcesses/KernelFunctions.jl/blob/gh-pages/v0.10.11/examples/kernel-ridge-regression.ipynb"><img src="https://img.shields.io/badge/show-nbviewer-579ACA.svg" alt/></a></p><p><em>You are seeing the HTML output generated by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and <a href="https://github.com/fredrikekre/Literate.jl">Literate.jl</a> from the <a href="https://github.com/JuliaGaussianProcesses/KernelFunctions.jl/blob/master/examples/kernel-ridge-regression/script.jl">Julia source file</a>. The corresponding notebook can be viewed in <a href="https://nbviewer.jupyter.org/github/JuliaGaussianProcesses/KernelFunctions.jl/blob/gh-pages/v0.10.11/examples/kernel-ridge-regression.ipynb">nbviewer</a>.</em></p><p>Building on linear regression, we can fit non-linear data sets by introducing a feature space. In a higher-dimensional feature space, we can overfit the data; ridge regression introduces regularization to avoid this. In this notebook we show how we can use KernelFunctions.jl for <em>kernel</em> ridge regression.</p><pre><code class="language-julia"># Loading and setup of required packages
using KernelFunctions
using LinearAlgebra
using Distributions

# Plotting
using Plots;
default(; lw=2.0, legendfontsize=11.0, ylims=(-150, 500));

using Random: seed!
seed!(42);</code></pre><h2 id="Toy-data"><a class="docs-heading-anchor" href="#Toy-data">Toy data</a><a id="Toy-data-1"></a><a class="docs-heading-anchor-permalink" href="#Toy-data" title="Permalink"></a></h2><p>Here we use a one-dimensional toy problem. We generate data using the fourth-order polynomial <span>$f(x) = (x+4)(x+1)(x-1)(x-3)$</span>:</p><pre><code class="language-julia">f_truth(x) = (x + 4) * (x + 1) * (x - 1) * (x - 3)

x_train = -5:0.5:5
x_test = -7:0.1:7

noise = rand(Uniform(-20, 20), length(x_train))
y_train = f_truth.(x_train) + noise
y_test = f_truth.(x_test)

plot(x_test, y_test; label=raw&quot;$f(x)$&quot;)
scatter!(x_train, y_train; seriescolor=1, label=&quot;observations&quot;)</code></pre><p><img src="../1776857873.png" alt/></p><h2 id="Linear-regression"><a class="docs-heading-anchor" href="#Linear-regression">Linear regression</a><a id="Linear-regression-1"></a><a class="docs-heading-anchor-permalink" href="#Linear-regression" title="Permalink"></a></h2><p>For training inputs <span>$\mathrm{X}=(\mathbf{x}_n)_{n=1}^N$</span> and observations <span>$\mathbf{y}=(y_n)_{n=1}^N$</span>, the linear regression weights <span>$\mathbf{w}$</span> using the least-squares estimator are given by</p><p class="math-container">\[\mathbf{w} = (\mathrm{X}^\top \mathrm{X})^{-1} \mathrm{X}^\top \mathbf{y}\]</p><p>We predict at test inputs <span>$\mathbf{x}_*$</span> using</p><p class="math-container">\[\hat{y}_* = \mathbf{x}_*^\top \mathbf{w}\]</p><p>This is implemented by <code>linear_regression</code>:</p><pre><code class="language-julia">function linear_regression(X, y, Xstar)
    weights = (X&#39; * X) \ (X&#39; * y)
    return Xstar * weights
end;</code></pre><p>A linear regression fit to the above data set:</p><pre><code class="language-julia">y_pred = linear_regression(x_train, y_train, x_test)
scatter(x_train, y_train; label=&quot;observations&quot;)
plot!(x_test, y_pred; label=&quot;linear fit&quot;)</code></pre><p><img src="../4285469668.png" alt/></p><h2 id="Featurization"><a class="docs-heading-anchor" href="#Featurization">Featurization</a><a id="Featurization-1"></a><a class="docs-heading-anchor-permalink" href="#Featurization" title="Permalink"></a></h2><p>We can improve the fit by including additional features, i.e. generalizing to <span>$\tilde{\mathrm{X}} = (\phi(x_n))_{n=1}^N$</span>, where <span>$\phi(x)$</span> constructs a feature vector for each input <span>$x$</span>. Here we include powers of the input, <span>$\phi(x) = (1, x, x^2, \dots, x^d)$</span>:</p><pre><code class="language-julia">function featurize_poly(x; degree=1)
    return repeat(x, 1, degree + 1) .^ (0:degree)&#39;
end

function featurized_fit_and_plot(degree)
    X = featurize_poly(x_train; degree=degree)
    Xstar = featurize_poly(x_test; degree=degree)
    y_pred = linear_regression(X, y_train, Xstar)
    scatter(x_train, y_train; legend=false, title=&quot;fit of order $degree&quot;)
    return plot!(x_test, y_pred)
end

plot((featurized_fit_and_plot(degree) for degree in 1:4)...)</code></pre><p><img src="../2146522050.png" alt/></p><p>Note that the fit becomes perfect when we include exactly as many orders in the features as we have in the underlying polynomial (4).</p><p>However, when increasing the number of features, we can quickly overfit to noise in the data set:</p><pre><code class="language-julia">featurized_fit_and_plot(20)</code></pre><p><img src="../2018359371.png" alt/></p><h2 id="Ridge-regression"><a class="docs-heading-anchor" href="#Ridge-regression">Ridge regression</a><a id="Ridge-regression-1"></a><a class="docs-heading-anchor-permalink" href="#Ridge-regression" title="Permalink"></a></h2><p>To counteract this unwanted behaviour, we can introduce regularization. This leads to <em>ridge regression</em> with <span>$L_2$</span> regularization of the weights (<a href="https://en.wikipedia.org/wiki/Tikhonov_regularization">Tikhonov regularization</a>). Instead of the weights in linear regression,</p><p class="math-container">\[\mathbf{w} = (\mathrm{X}^\top \mathrm{X})^{-1} \mathrm{X}^\top \mathbf{y}\]</p><p>we introduce the ridge parameter <span>$\lambda$</span>:</p><p class="math-container">\[\mathbf{w} = (\mathrm{X}^\top \mathrm{X} + \lambda \mathbb{1})^{-1} \mathrm{X}^\top \mathbf{y}\]</p><p>As before, we predict at test inputs <span>$\mathbf{x}_*$</span> using</p><p class="math-container">\[\hat{y}_* = \mathbf{x}_*^\top \mathbf{w}\]</p><p>This is implemented by <code>ridge_regression</code>:</p><pre><code class="language-julia">function ridge_regression(X, y, Xstar, lambda)
    weights = (X&#39; * X + lambda * I) \ (X&#39; * y)
    return Xstar * weights
end

function regularized_fit_and_plot(degree, lambda)
    X = featurize_poly(x_train; degree=degree)
    Xstar = featurize_poly(x_test; degree=degree)
    y_pred = ridge_regression(X, y_train, Xstar, lambda)
    scatter(x_train, y_train; legend=false, title=&quot;\$\\lambda=$lambda\$&quot;)
    return plot!(x_test, y_pred)
end

plot((regularized_fit_and_plot(20, lambda) for lambda in (1e-3, 1e-2, 1e-1, 1))...)</code></pre><p><img src="../2116461788.png" alt/></p><h2 id="Kernel-ridge-regression"><a class="docs-heading-anchor" href="#Kernel-ridge-regression">Kernel ridge regression</a><a id="Kernel-ridge-regression-1"></a><a class="docs-heading-anchor-permalink" href="#Kernel-ridge-regression" title="Permalink"></a></h2><p>Instead of constructing the feature matrix explicitly, we can use <em>kernels</em> to replace inner products of feature vectors with a kernel evaluation: <span>$\langle \phi(x), \phi(x&#39;) \rangle = k(x, x&#39;)$</span> or <span>$\tilde{\mathrm{X}} \tilde{\mathrm{X}}^\top = \mathrm{K}$</span>, where <span>$\mathrm{K}_{ij} = k(x_i, x_j)$</span>.</p><p>To apply this &quot;kernel trick&quot; to ridge regression, we can rewrite the ridge estimate for the weights</p><p class="math-container">\[\mathbf{w} = (\mathrm{X}^\top \mathrm{X} + \lambda \mathbb{1})^{-1} \mathrm{X}^\top \mathbf{y}\]</p><p>using the <a href="https://tlienart.github.io/pub/csml/mtheory/matinvlem.html#basic_lemmas">matrix inversion lemma</a> as</p><p class="math-container">\[\mathbf{w} = \mathrm{X}^\top (\mathrm{X} \mathrm{X}^\top + \lambda \mathbb{1})^{-1} \mathbf{y}\]</p><p>where we can now replace the inner product with the kernel matrix,</p><p class="math-container">\[\mathbf{w} = \mathrm{X}^\top (\mathrm{K} + \lambda \mathbb{1})^{-1} \mathbf{y}\]</p><p>And the prediction yields another inner product,</p><p class="math-container">\[\hat{y}_* = \mathbf{x}_*^\top \mathbf{w} = \langle \mathbf{x}_*, \mathbf{w} \rangle = \mathbf{k}_* (\mathrm{K} + \lambda \mathbb{1})^{-1} \mathbf{y}\]</p><p>where <span>$(\mathbf{k}_*)_n = k(x_*, x_n)$</span>.</p><p>This is implemented by <code>kernel_ridge_regression</code>:</p><pre><code class="language-julia">function kernel_ridge_regression(k, X, y, Xstar, lambda)
    K = kernelmatrix(k, X)
    kstar = kernelmatrix(k, Xstar, X)
    return kstar * ((K + lambda * I) \ y)
end;</code></pre><p>Now, instead of explicitly constructing features, we can simply pass in a <code>PolynomialKernel</code> object:</p><pre><code class="language-julia">function kernelized_fit_and_plot(kernel, lambda=1e-4)
    y_pred = kernel_ridge_regression(kernel, x_train, y_train, x_test, lambda)
    if kernel isa PolynomialKernel
        title = string(&quot;order &quot;, kernel.degree)
    else
        title = string(nameof(typeof(kernel)))
    end
    scatter(x_train, y_train; label=nothing)
    return plot!(x_test, y_pred; label=nothing, title=title)
end

plot((kernelized_fit_and_plot(PolynomialKernel(; degree=degree, c=1)) for degree in 1:4)...)</code></pre><p><img src="../3484870383.png" alt/></p><p>However, we can now also use kernels that would have an infinite-dimensional feature expansion, such as the squared exponential kernel:</p><pre><code class="language-julia">kernelized_fit_and_plot(SqExponentialKernel())</code></pre><p><img src="../3039720541.png" alt/></p><hr/><p><em>This page was generated using <a href="https://github.com/fredrikekre/Literate.jl">Literate.jl</a>.</em></p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../gaussian-process-priors/">« Gaussian process prior samples</a><a class="docs-footer-nextpage" href="../support-vector-machine/">Support Vector Machine »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.3 on <span class="colophon-date" title="Friday 30 July 2021 11:42">Friday 30 July 2021</span>. Using Julia version 1.6.2.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
